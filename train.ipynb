{"cells":[{"cell_type":"markdown","metadata":{"id":"SwkFEsEgCCyl"},"source":["# Foursquare dataset next-POI Recommendation System"]},{"cell_type":"markdown","metadata":{"id":"MZnW-ffHCCym"},"source":["First off we import all the necessary libraries:"]},{"cell_type":"code","execution_count":162,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7ML-ffMMCCyn","outputId":"075952a5-3d41-4594-8495-97fba5697bcb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: lightning in /usr/local/lib/python3.10/dist-packages (2.2.4)\n","\n","Requirement already satisfied: geohash2 in /usr/local/lib/python3.10/dist-packages (1.1)\n","\n","Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.17.0)\n","\n","Requirement already satisfied: polars==0.20.25 in /usr/local/lib/python3.10/dist-packages (0.20.25)\n","\n","Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n","\n","Requirement already satisfied: fsspec[http]<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2023.6.0)\n","\n","Requirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (0.11.2)\n","\n","Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.25.2)\n","\n","Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (24.0)\n","\n","Requirement already satisfied: torch<4.0,>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.2.1+cu121)\n","\n","Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.4.0.post0)\n","\n","Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.4)\n","\n","Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.11.0)\n","\n","Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning) (2.2.4)\n","\n","Requirement already satisfied: docutils>=0.3 in /usr/local/lib/python3.10/dist-packages (from geohash2) (0.18.1)\n","\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n","\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n","\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n","\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.2.1)\n","\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n","\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.2.0)\n","\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n","\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning) (3.9.5)\n","\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n","\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n","\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.7)\n","\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n","\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n","\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.14.0)\n","\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (1.12)\n","\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.3)\n","\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (3.1.4)\n","\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (12.1.105)\n","\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (12.1.105)\n","\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (12.1.105)\n","\n","Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (8.9.2.26)\n","\n","Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (12.1.3.1)\n","\n","Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (11.0.2.54)\n","\n","Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (10.3.2.106)\n","\n","Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (11.4.5.107)\n","\n","Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (12.1.0.106)\n","\n","Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (2.19.3)\n","\n","Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (12.1.105)\n","\n","Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.13.0->lightning) (2.2.0)\n","\n","Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<4.0,>=1.13.0->lightning) (12.4.127)\n","\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.3.1)\n","\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (23.2.0)\n","\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.4.1)\n","\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (6.0.5)\n","\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (1.9.4)\n","\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning) (4.0.3)\n","\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n","\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=1.13.0->lightning) (2.1.5)\n","\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.13.0->lightning) (1.3.0)\n"]}],"source":[" %pip install lightning geohash2 wandb polars==0.20.25"]},{"cell_type":"code","execution_count":163,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e7nPgeM4CLmX","outputId":"b786e779-2bd5-4d98-fc0b-9d7a6769bdd0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":[" #from google.colab import drive\n","\n"," #drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":164,"metadata":{"id":"Z8xK-4YqCCyn"},"outputs":[],"source":["import polars as rs\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import lightning as pl\n","import lightning.pytorch as torchpl\n","from tqdm import tqdm\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","from dataclasses import dataclass\n","import wandb\n","from rich import print"]},{"cell_type":"code","execution_count":165,"metadata":{"id":"FoCozR1YCCyo"},"outputs":[],"source":["import os\n","\n","# define WANDB_NOTEBOOK_NAME\n","os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"train.ipynb\""]},{"cell_type":"code","execution_count":166,"metadata":{"id":"Yq1AScVfCCyo"},"outputs":[],"source":["import gc\n","\n","gc.collect()\n","# clean CUDA memory\n","torch.cuda.empty_cache()\n","\n","# sometimes jupyter notebook does not release memory, we leave this here so a run-all\n","# can *sometimes* fix leaks"]},{"cell_type":"markdown","metadata":{"id":"Z402h6bPCCyo"},"source":["Next, we load the data, we utilize `polars` since it is much more efficient than `pandas` and can handle large datasets with ease."]},{"cell_type":"code","execution_count":167,"metadata":{"id":"1F7L_QPkCCyo"},"outputs":[],"source":["columns = [\"user\", \"poi\", \"date\", \"TZ\"]\n","\n","DATASET_PATH = \"/kaggle/input/dataset-tist2015/dataset_TIST2015_Checkins.txt\"\n","\n","data = rs.read_csv(\n","    DATASET_PATH,\n","    has_header=False,\n","    low_memory=True,\n","    separator=\"\\t\",\n",")\n","data.columns = columns"]},{"cell_type":"code","execution_count":168,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"LmYprUhOCCyo","outputId":"66033073-697a-4491-eb63-a1780ff90091"},"outputs":[{"data":{"text/html":["<div><style>\n",".dataframe > thead > tr,\n",".dataframe > tbody > tr {\n","  text-align: right;\n","  white-space: pre-wrap;\n","}\n","</style>\n","<small>shape: (33_263_633, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user</th><th>poi</th><th>date</th><th>TZ</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>50756</td><td>&quot;4f5e3a72e4b053fd6a4313f6&quot;</td><td>&quot;Tue Apr 03 18:00:06 +0000 2012&quot;</td><td>240</td></tr><tr><td>190571</td><td>&quot;4b4b87b5f964a5204a9f26e3&quot;</td><td>&quot;Tue Apr 03 18:00:07 +0000 2012&quot;</td><td>180</td></tr><tr><td>221021</td><td>&quot;4a85b1b3f964a520eefe1fe3&quot;</td><td>&quot;Tue Apr 03 18:00:08 +0000 2012&quot;</td><td>-240</td></tr><tr><td>66981</td><td>&quot;4b4606f2f964a520751426e3&quot;</td><td>&quot;Tue Apr 03 18:00:08 +0000 2012&quot;</td><td>-300</td></tr><tr><td>21010</td><td>&quot;4c2b4e8a9a559c74832f0de2&quot;</td><td>&quot;Tue Apr 03 18:00:09 +0000 2012&quot;</td><td>240</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>16349</td><td>&quot;4c957755c8a1bfb7e89024f3&quot;</td><td>&quot;Mon Sep 16 23:24:11 +0000 2013&quot;</td><td>-240</td></tr><tr><td>256757</td><td>&quot;4c8bbb6d9ef0224bd2d6667b&quot;</td><td>&quot;Mon Sep 16 23:24:13 +0000 2013&quot;</td><td>-180</td></tr><tr><td>66425</td><td>&quot;513e82a5e4b0ed4f0f3bcf2d&quot;</td><td>&quot;Mon Sep 16 23:24:14 +0000 2013&quot;</td><td>-180</td></tr><tr><td>1830</td><td>&quot;4b447865f964a5204cf525e3&quot;</td><td>&quot;Mon Sep 16 23:24:14 +0000 2013&quot;</td><td>120</td></tr><tr><td>22704</td><td>&quot;50df4ee5e4b0c48b5a1c2968&quot;</td><td>&quot;Mon Sep 16 23:24:15 +0000 2013&quot;</td><td>180</td></tr></tbody></table></div>"],"text/plain":["shape: (33_263_633, 4)\n","┌────────┬──────────────────────────┬────────────────────────────────┬──────┐\n","│ user   ┆ poi                      ┆ date                           ┆ TZ   │\n","│ ---    ┆ ---                      ┆ ---                            ┆ ---  │\n","│ i64    ┆ str                      ┆ str                            ┆ i64  │\n","╞════════╪══════════════════════════╪════════════════════════════════╪══════╡\n","│ 50756  ┆ 4f5e3a72e4b053fd6a4313f6 ┆ Tue Apr 03 18:00:06 +0000 2012 ┆ 240  │\n","│ 190571 ┆ 4b4b87b5f964a5204a9f26e3 ┆ Tue Apr 03 18:00:07 +0000 2012 ┆ 180  │\n","│ 221021 ┆ 4a85b1b3f964a520eefe1fe3 ┆ Tue Apr 03 18:00:08 +0000 2012 ┆ -240 │\n","│ 66981  ┆ 4b4606f2f964a520751426e3 ┆ Tue Apr 03 18:00:08 +0000 2012 ┆ -300 │\n","│ 21010  ┆ 4c2b4e8a9a559c74832f0de2 ┆ Tue Apr 03 18:00:09 +0000 2012 ┆ 240  │\n","│ …      ┆ …                        ┆ …                              ┆ …    │\n","│ 16349  ┆ 4c957755c8a1bfb7e89024f3 ┆ Mon Sep 16 23:24:11 +0000 2013 ┆ -240 │\n","│ 256757 ┆ 4c8bbb6d9ef0224bd2d6667b ┆ Mon Sep 16 23:24:13 +0000 2013 ┆ -180 │\n","│ 66425  ┆ 513e82a5e4b0ed4f0f3bcf2d ┆ Mon Sep 16 23:24:14 +0000 2013 ┆ -180 │\n","│ 1830   ┆ 4b447865f964a5204cf525e3 ┆ Mon Sep 16 23:24:14 +0000 2013 ┆ 120  │\n","│ 22704  ┆ 50df4ee5e4b0c48b5a1c2968 ┆ Mon Sep 16 23:24:15 +0000 2013 ┆ 180  │\n","└────────┴──────────────────────────┴────────────────────────────────┴──────┘"]},"execution_count":168,"metadata":{},"output_type":"execute_result"}],"source":["data"]},{"cell_type":"markdown","metadata":{"id":"AE8Cxd2PCCyo"},"source":["Differently from what suggested by the professor, we utilize the full TIST2015 dataset, which has a far greater scale compared to the reduced NY one. However, by following the pruning steps detailed in the paper (http://dx.doi.org/10.1145/3477495.3531989, section 5.1), we obtain sequences that are much smaller in size, resulting in a dataset that is usable on Google Colab's free tier (as required by the assignment)."]},{"cell_type":"code","execution_count":169,"metadata":{"id":"YqLUIt0cCCyo"},"outputs":[],"source":["data_users = (\n","    data.lazy()\n","    .group_by(\"user\")\n","    .agg(\n","        [\n","            rs.col(\"poi\").n_unique().alias(\"n_pois\"),\n","            rs.col(\"poi\").count().alias(\"n_checkins\"),\n","            # turn the rest into a list\n","            rs.col(\"poi\").alias(\"pois\"),\n","            rs.col(\"date\").alias(\"dates\"),\n","            rs.col(\"TZ\").alias(\"TZs\"),\n","        ]\n","    )\n",").collect()"]},{"cell_type":"code","execution_count":170,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"id":"HptthFauCCyo","outputId":"3e24da12-5c14-43c6-a990-118f72ce0637"},"outputs":[{"data":{"text/html":["<div><style>\n",".dataframe > thead > tr,\n",".dataframe > tbody > tr {\n","  text-align: right;\n","  white-space: pre-wrap;\n","}\n","</style>\n","<small>shape: (9, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>user</th><th>n_pois</th><th>n_checkins</th><th>pois</th><th>dates</th><th>TZs</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>266909.0</td><td>266909.0</td><td>266909.0</td><td>266909.0</td><td>266909.0</td><td>266909.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>133455.0</td><td>56.477459</td><td>124.62537</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;std&quot;</td><td>77050.135837</td><td>45.968603</td><td>140.692138</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;min&quot;</td><td>1.0</td><td>1.0</td><td>1.0</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;25%&quot;</td><td>66728.0</td><td>30.0</td><td>61.0</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;50%&quot;</td><td>133455.0</td><td>49.0</td><td>93.0</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;75%&quot;</td><td>200182.0</td><td>71.0</td><td>148.0</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&quot;max&quot;</td><td>266909.0</td><td>1246.0</td><td>5430.0</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"],"text/plain":["shape: (9, 7)\n","┌────────────┬──────────────┬───────────┬────────────┬──────────┬──────────┬──────────┐\n","│ statistic  ┆ user         ┆ n_pois    ┆ n_checkins ┆ pois     ┆ dates    ┆ TZs      │\n","│ ---        ┆ ---          ┆ ---       ┆ ---        ┆ ---      ┆ ---      ┆ ---      │\n","│ str        ┆ f64          ┆ f64       ┆ f64        ┆ f64      ┆ f64      ┆ f64      │\n","╞════════════╪══════════════╪═══════════╪════════════╪══════════╪══════════╪══════════╡\n","│ count      ┆ 266909.0     ┆ 266909.0  ┆ 266909.0   ┆ 266909.0 ┆ 266909.0 ┆ 266909.0 │\n","│ null_count ┆ 0.0          ┆ 0.0       ┆ 0.0        ┆ 0.0      ┆ 0.0      ┆ 0.0      │\n","│ mean       ┆ 133455.0     ┆ 56.477459 ┆ 124.62537  ┆ null     ┆ null     ┆ null     │\n","│ std        ┆ 77050.135837 ┆ 45.968603 ┆ 140.692138 ┆ null     ┆ null     ┆ null     │\n","│ min        ┆ 1.0          ┆ 1.0       ┆ 1.0        ┆ null     ┆ null     ┆ null     │\n","│ 25%        ┆ 66728.0      ┆ 30.0      ┆ 61.0       ┆ null     ┆ null     ┆ null     │\n","│ 50%        ┆ 133455.0     ┆ 49.0      ┆ 93.0       ┆ null     ┆ null     ┆ null     │\n","│ 75%        ┆ 200182.0     ┆ 71.0      ┆ 148.0      ┆ null     ┆ null     ┆ null     │\n","│ max        ┆ 266909.0     ┆ 1246.0    ┆ 5430.0     ┆ null     ┆ null     ┆ null     │\n","└────────────┴──────────────┴───────────┴────────────┴──────────┴──────────┴──────────┘"]},"execution_count":170,"metadata":{},"output_type":"execute_result"}],"source":["data_users.describe()"]},{"cell_type":"markdown","metadata":{"id":"VqJjYuF1CCyp"},"source":["## Data Preprocessing"]},{"cell_type":"code","execution_count":171,"metadata":{"id":"SxCtvTymCCyp"},"outputs":[],"source":["data_culled = data_users.filter(\n","    (rs.col(\"n_checkins\") > 20) & (rs.col(\"n_checkins\") < 50)\n",").drop_nulls()"]},{"cell_type":"markdown","metadata":{"id":"rAqugUVTCCyp"},"source":["Since the original dataset is huge, we delete it and call the python garbage collector to free up memory. We then proceed with the second pruning step (frequency-based pruning) as detailed in the paper."]},{"cell_type":"code","execution_count":172,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vc26KLj7CCyp","outputId":"33a0fb07-2a8d-4067-c8c7-0aed57160466"},"outputs":[{"data":{"text/plain":["0"]},"execution_count":172,"metadata":{},"output_type":"execute_result"}],"source":["del data\n","del data_users\n","\n","import gc\n","\n","gc.collect()"]},{"cell_type":"code","execution_count":173,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"mcWNAa-HaItt","outputId":"adb2acf1-40db-4fbf-f025-53d30a228009"},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">21</span> <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">49</span>\n","</pre>\n"],"text/plain":["\u001b[1;36m21\u001b[0m \u001b[1;36m49\u001b[0m\n"]},"metadata":{},"output_type":"display_data"}],"source":["# print lengths\n","\n","print(data_culled[\"pois\"].list.len().min(), data_culled[\"pois\"].list.len().max())"]},{"cell_type":"code","execution_count":174,"metadata":{"id":"t9jLArwDCCyp"},"outputs":[],"source":["# extract unique elements from each lists in data_culled[\"pois\"]\n","out = data_culled.with_columns(\n","    [\n","        rs.col(\"pois\").list.unique(),\n","        rs.col(\"pois\").list.unique().list.len().alias(\"n_unique_pois\"),\n","    ]\n",")"]},{"cell_type":"code","execution_count":175,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":565},"id":"xmWP2h6uCCyp","outputId":"679b33b6-5f50-413d-e681-2f6d55fa5911"},"outputs":[{"data":{"text/html":["<div><style>\n",".dataframe > thead > tr,\n",".dataframe > tbody > tr {\n","  text-align: right;\n","  white-space: pre-wrap;\n","}\n","</style>\n","<small>shape: (21_697, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user</th><th>n_pois</th><th>n_checkins</th><th>pois</th><th>dates</th><th>TZs</th><th>n_unique_pois</th></tr><tr><td>i64</td><td>u32</td><td>u32</td><td>list[str]</td><td>list[str]</td><td>list[i64]</td><td>u32</td></tr></thead><tbody><tr><td>81801</td><td>18</td><td>27</td><td>[&quot;4c060b308f8fa593e4bdf20d&quot;, &quot;4b5dd035f964a520296d29e3&quot;, … &quot;4bcd81f6b6c49c744b6d9591&quot;]</td><td>[&quot;Sun Jun 10 04:15:00 +0000 2012&quot;, &quot;Sun Jun 10 04:15:10 +0000 2012&quot;, … &quot;Tue Jun 11 16:15:50 +0000 2013&quot;]</td><td>[120, 120, … 120]</td><td>18</td></tr><tr><td>86919</td><td>32</td><td>38</td><td>[&quot;4bf53cf6cad2c9284fc59c99&quot;, &quot;4b5b5871f964a5205ff628e3&quot;, … &quot;4bbf5b9598f49521b87fd263&quot;]</td><td>[&quot;Mon Jul 02 19:19:07 +0000 2012&quot;, &quot;Mon Jul 02 19:20:31 +0000 2012&quot;, … &quot;Sat Aug 24 23:58:37 +0000 2013&quot;]</td><td>[-240, -240, … -240]</td><td>32</td></tr><tr><td>85865</td><td>24</td><td>34</td><td>[&quot;4b058715f964a520e17e22e3&quot;, &quot;4bdee9f20ee3a593f94a32b0&quot;, … &quot;4f8f04efe4b09b4d928637a4&quot;]</td><td>[&quot;Sat May 05 14:35:08 +0000 2012&quot;, &quot;Sat May 05 17:00:45 +0000 2012&quot;, … &quot;Sun Jun 09 07:10:10 +0000 2013&quot;]</td><td>[-180, -180, … -180]</td><td>24</td></tr><tr><td>200312</td><td>31</td><td>41</td><td>[&quot;4ee4d8f3d3e34ebcf036cfd3&quot;, &quot;4cd149843e63721ee0a9a2cc&quot;, … &quot;45587735f964a520443d1fe3&quot;]</td><td>[&quot;Fri Oct 19 14:51:57 +0000 2012&quot;, &quot;Fri Oct 19 16:52:58 +0000 2012&quot;, … &quot;Sat Sep 14 15:44:27 +0000 2013&quot;]</td><td>[60, 60, … -240]</td><td>31</td></tr><tr><td>209485</td><td>28</td><td>29</td><td>[&quot;4b07c29cf964a520d8ff22e3&quot;, &quot;4f114fa3e4b067f66f72a61e&quot;, … &quot;4c5d02cc6147be9a11159109&quot;]</td><td>[&quot;Sat Apr 14 21:57:08 +0000 2012&quot;, &quot;Sat Apr 14 22:16:19 +0000 2012&quot;, … &quot;Sun Apr 29 14:11:12 +0000 2012&quot;]</td><td>[540, 540, … 540]</td><td>28</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>246290</td><td>31</td><td>39</td><td>[&quot;4a5e33adf964a52025be1fe3&quot;, &quot;4aa801bff964a520aa4e20e3&quot;, … &quot;4b1c80c5f964a520f50724e3&quot;]</td><td>[&quot;Thu Jul 05 18:47:36 +0000 2012&quot;, &quot;Fri Jul 06 18:45:10 +0000 2012&quot;, … &quot;Sat Mar 16 22:11:11 +0000 2013&quot;]</td><td>[-420, -420, … -420]</td><td>31</td></tr><tr><td>123605</td><td>26</td><td>44</td><td>[&quot;4c7c42f509b29c74f0b0e03c&quot;, &quot;4b5689b6f964a5206d1428e3&quot;, … &quot;4bde03b30ee3a593eb9a2fb0&quot;]</td><td>[&quot;Tue Apr 03 18:16:51 +0000 2012&quot;, &quot;Sat Apr 07 15:08:17 +0000 2012&quot;, … &quot;Sun Sep 09 13:57:18 +0000 2012&quot;]</td><td>[-300, -300, … -300]</td><td>26</td></tr><tr><td>246013</td><td>21</td><td>29</td><td>[&quot;4b15507bf964a520a7b023e3&quot;, &quot;4b21e5d4f964a520444224e3&quot;, … &quot;4b497b00f964a520637026e3&quot;]</td><td>[&quot;Sun May 27 23:30:46 +0000 2012&quot;, &quot;Wed May 30 01:53:48 +0000 2012&quot;, … &quot;Sun Aug 04 22:33:55 +0000 2013&quot;]</td><td>[-300, -300, … -360]</td><td>21</td></tr><tr><td>98164</td><td>32</td><td>37</td><td>[&quot;4da052d7784f3704a8bf96af&quot;, &quot;4e74a8201838f918897c9818&quot;, … &quot;4da43bb2d686b60c6f36db28&quot;]</td><td>[&quot;Wed May 23 17:57:42 +0000 2012&quot;, &quot;Wed May 23 18:16:45 +0000 2012&quot;, … &quot;Fri Jul 05 22:50:16 +0000 2013&quot;]</td><td>[-180, -180, … -180]</td><td>32</td></tr><tr><td>51894</td><td>33</td><td>42</td><td>[&quot;4c6fe0c39c6d6dcb21b3d07a&quot;, &quot;4b780e35f964a520c0b32ee3&quot;, … &quot;4c680e06e1da1b8dc10b9fc3&quot;]</td><td>[&quot;Sun Jul 08 07:13:26 +0000 2012&quot;, &quot;Sun Jul 15 14:07:35 +0000 2012&quot;, … &quot;Sat Aug 31 10:34:28 +0000 2013&quot;]</td><td>[180, 180, … 180]</td><td>33</td></tr></tbody></table></div>"],"text/plain":["shape: (21_697, 7)\n","┌────────┬────────┬────────────┬─────────────────┬────────────────┬────────────────┬───────────────┐\n","│ user   ┆ n_pois ┆ n_checkins ┆ pois            ┆ dates          ┆ TZs            ┆ n_unique_pois │\n","│ ---    ┆ ---    ┆ ---        ┆ ---             ┆ ---            ┆ ---            ┆ ---           │\n","│ i64    ┆ u32    ┆ u32        ┆ list[str]       ┆ list[str]      ┆ list[i64]      ┆ u32           │\n","╞════════╪════════╪════════════╪═════════════════╪════════════════╪════════════════╪═══════════════╡\n","│ 81801  ┆ 18     ┆ 27         ┆ [\"4c060b308f8fa ┆ [\"Sun Jun 10   ┆ [120, 120, …   ┆ 18            │\n","│        ┆        ┆            ┆ 593e4bdf20d\",   ┆ 04:15:00 +0000 ┆ 120]           ┆               │\n","│        ┆        ┆            ┆ \"…              ┆ 20…            ┆                ┆               │\n","│ 86919  ┆ 32     ┆ 38         ┆ [\"4bf53cf6cad2c ┆ [\"Mon Jul 02   ┆ [-240, -240, … ┆ 32            │\n","│        ┆        ┆            ┆ 9284fc59c99\",   ┆ 19:19:07 +0000 ┆ -240]          ┆               │\n","│        ┆        ┆            ┆ \"…              ┆ 20…            ┆                ┆               │\n","│ 85865  ┆ 24     ┆ 34         ┆ [\"4b058715f964a ┆ [\"Sat May 05   ┆ [-180, -180, … ┆ 24            │\n","│        ┆        ┆            ┆ 520e17e22e3\",   ┆ 14:35:08 +0000 ┆ -180]          ┆               │\n","│        ┆        ┆            ┆ \"…              ┆ 20…            ┆                ┆               │\n","│ 200312 ┆ 31     ┆ 41         ┆ [\"4ee4d8f3d3e34 ┆ [\"Fri Oct 19   ┆ [60, 60, …     ┆ 31            │\n","│        ┆        ┆            ┆ ebcf036cfd3\",   ┆ 14:51:57 +0000 ┆ -240]          ┆               │\n","│        ┆        ┆            ┆ \"…              ┆ 20…            ┆                ┆               │\n","│ 209485 ┆ 28     ┆ 29         ┆ [\"4b07c29cf964a ┆ [\"Sat Apr 14   ┆ [540, 540, …   ┆ 28            │\n","│        ┆        ┆            ┆ 520d8ff22e3\",   ┆ 21:57:08 +0000 ┆ 540]           ┆               │\n","│        ┆        ┆            ┆ \"…              ┆ 20…            ┆                ┆               │\n","│ …      ┆ …      ┆ …          ┆ …               ┆ …              ┆ …              ┆ …             │\n","│ 246290 ┆ 31     ┆ 39         ┆ [\"4a5e33adf964a ┆ [\"Thu Jul 05   ┆ [-420, -420, … ┆ 31            │\n","│        ┆        ┆            ┆ 52025be1fe3\",   ┆ 18:47:36 +0000 ┆ -420]          ┆               │\n","│        ┆        ┆            ┆ \"…              ┆ 20…            ┆                ┆               │\n","│ 123605 ┆ 26     ┆ 44         ┆ [\"4c7c42f509b29 ┆ [\"Tue Apr 03   ┆ [-300, -300, … ┆ 26            │\n","│        ┆        ┆            ┆ c74f0b0e03c\",   ┆ 18:16:51 +0000 ┆ -300]          ┆               │\n","│        ┆        ┆            ┆ \"…              ┆ 20…            ┆                ┆               │\n","│ 246013 ┆ 21     ┆ 29         ┆ [\"4b15507bf964a ┆ [\"Sun May 27   ┆ [-300, -300, … ┆ 21            │\n","│        ┆        ┆            ┆ 520a7b023e3\",   ┆ 23:30:46 +0000 ┆ -360]          ┆               │\n","│        ┆        ┆            ┆ \"…              ┆ 20…            ┆                ┆               │\n","│ 98164  ┆ 32     ┆ 37         ┆ [\"4da052d7784f3 ┆ [\"Wed May 23   ┆ [-180, -180, … ┆ 32            │\n","│        ┆        ┆            ┆ 704a8bf96af\",   ┆ 17:57:42 +0000 ┆ -180]          ┆               │\n","│        ┆        ┆            ┆ \"…              ┆ 20…            ┆                ┆               │\n","│ 51894  ┆ 33     ┆ 42         ┆ [\"4c6fe0c39c6d6 ┆ [\"Sun Jul 08   ┆ [180, 180, …   ┆ 33            │\n","│        ┆        ┆            ┆ dcb21b3d07a\",   ┆ 07:13:26 +0000 ┆ 180]           ┆               │\n","│        ┆        ┆            ┆ \"…              ┆ 20…            ┆                ┆               │\n","└────────┴────────┴────────────┴─────────────────┴────────────────┴────────────────┴───────────────┘"]},"execution_count":175,"metadata":{},"output_type":"execute_result"}],"source":["out"]},{"cell_type":"code","execution_count":176,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BTYdomCBCCyp","outputId":"b2a65bb5-4032-4c09-cef2-d4fb8ce8bad8"},"outputs":[{"data":{"text/plain":["18"]},"execution_count":176,"metadata":{},"output_type":"execute_result"}],"source":["l = out[\"pois\"][0].to_list()\n","len(set(l))  # print number of unique POIs in first sequence"]},{"cell_type":"code","execution_count":177,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ZvejeoKCCyp","outputId":"7208d743-35b8-4f3c-8d98-6189d1463d1e"},"outputs":[{"data":{"text/plain":["27"]},"execution_count":177,"metadata":{},"output_type":"execute_result"}],"source":["l2 = data_culled[\"pois\"][0].to_list()\n","len(l2)  # print sequence length of first user"]},{"cell_type":"code","execution_count":178,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8f24-SOxCCyp","outputId":"6b8b39d0-7172-4262-f2b4-728ab287d75d"},"outputs":[{"data":{"text/plain":["18"]},"execution_count":178,"metadata":{},"output_type":"execute_result"}],"source":["len(set(l2))  # confirm that the two match"]},{"cell_type":"code","execution_count":179,"metadata":{"id":"i51vz46MCCyp"},"outputs":[],"source":["# run a Polars query to obtain all the frequent POIs, the ones expected to survive the filtering\n","unique_pois = out[\"pois\"]\n","frequent_pois = unique_pois.list.explode().value_counts().filter(rs.col(\"count\") >= 10)"]},{"cell_type":"code","execution_count":180,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"7aHVCPtcCCyp","outputId":"9f59d9c4-66fe-457b-cce9-2431c21dc506"},"outputs":[{"data":{"text/html":["<div><style>\n",".dataframe > thead > tr,\n",".dataframe > tbody > tr {\n","  text-align: right;\n","  white-space: pre-wrap;\n","}\n","</style>\n","<small>shape: (4_455, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>pois</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;4d286632849f37044d4d7341&quot;</td><td>10</td></tr><tr><td>&quot;4b05864ff964a520385b22e3&quot;</td><td>11</td></tr><tr><td>&quot;4db45a5f1e7248d1359fb618&quot;</td><td>44</td></tr><tr><td>&quot;43519800f964a520bd281fe3&quot;</td><td>11</td></tr><tr><td>&quot;4bea177d62c0c9286088e0d4&quot;</td><td>28</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;4aef26ddf964a52019d621e3&quot;</td><td>17</td></tr><tr><td>&quot;4b5fdf6cf964a5205acf29e3&quot;</td><td>90</td></tr><tr><td>&quot;4f45df8ce4b03c926b3159e3&quot;</td><td>10</td></tr><tr><td>&quot;4c48305e96abd13aa25e7301&quot;</td><td>33</td></tr><tr><td>&quot;4da9a1ac93a04642f06a9b56&quot;</td><td>11</td></tr></tbody></table></div>"],"text/plain":["shape: (4_455, 2)\n","┌──────────────────────────┬───────┐\n","│ pois                     ┆ count │\n","│ ---                      ┆ ---   │\n","│ str                      ┆ u32   │\n","╞══════════════════════════╪═══════╡\n","│ 4d286632849f37044d4d7341 ┆ 10    │\n","│ 4b05864ff964a520385b22e3 ┆ 11    │\n","│ 4db45a5f1e7248d1359fb618 ┆ 44    │\n","│ 43519800f964a520bd281fe3 ┆ 11    │\n","│ 4bea177d62c0c9286088e0d4 ┆ 28    │\n","│ …                        ┆ …     │\n","│ 4aef26ddf964a52019d621e3 ┆ 17    │\n","│ 4b5fdf6cf964a5205acf29e3 ┆ 90    │\n","│ 4f45df8ce4b03c926b3159e3 ┆ 10    │\n","│ 4c48305e96abd13aa25e7301 ┆ 33    │\n","│ 4da9a1ac93a04642f06a9b56 ┆ 11    │\n","└──────────────────────────┴───────┘"]},"execution_count":180,"metadata":{},"output_type":"execute_result"}],"source":["frequent_pois"]},{"cell_type":"code","execution_count":181,"metadata":{"id":"34H0_RIbCCyp"},"outputs":[],"source":["frequent_pois = frequent_pois[\"pois\"]\n","frequent_pois = set(frequent_pois.to_list())"]},{"cell_type":"code","execution_count":182,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"ib_qORq7CCyp","outputId":"fbc3d319-4cdc-418d-cb5e-a8741596d1d6"},"outputs":[{"data":{"text/html":["<div><style>\n",".dataframe > thead > tr,\n",".dataframe > tbody > tr {\n","  text-align: right;\n","  white-space: pre-wrap;\n","}\n","</style>\n","<small>shape: (21_697, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user</th><th>n_pois</th><th>n_checkins</th><th>pois</th><th>dates</th><th>TZs</th></tr><tr><td>i64</td><td>u32</td><td>u32</td><td>list[str]</td><td>list[str]</td><td>list[i64]</td></tr></thead><tbody><tr><td>81801</td><td>18</td><td>27</td><td>[&quot;4bac6e11f964a520c8f43ae3&quot;, &quot;4adcda21f964a520ff3921e3&quot;, … &quot;4c060b308f8fa593e4bdf20d&quot;]</td><td>[&quot;Sun Jun 10 04:15:00 +0000 2012&quot;, &quot;Sun Jun 10 04:15:10 +0000 2012&quot;, … &quot;Tue Jun 11 16:15:50 +0000 2013&quot;]</td><td>[120, 120, … 120]</td></tr><tr><td>86919</td><td>32</td><td>38</td><td>[&quot;4b8567f2f964a520b85b31e3&quot;, &quot;4bbd5c994e069c74d5be9ee3&quot;, … &quot;4e3157e97d8b9b256bca2e77&quot;]</td><td>[&quot;Mon Jul 02 19:19:07 +0000 2012&quot;, &quot;Mon Jul 02 19:20:31 +0000 2012&quot;, … &quot;Sat Aug 24 23:58:37 +0000 2013&quot;]</td><td>[-240, -240, … -240]</td></tr><tr><td>85865</td><td>24</td><td>34</td><td>[&quot;4bb665b86edc76b001e0301c&quot;, &quot;4b10649ff964a520706f23e3&quot;, … &quot;4b3c1611f964a520978125e3&quot;]</td><td>[&quot;Sat May 05 14:35:08 +0000 2012&quot;, &quot;Sat May 05 17:00:45 +0000 2012&quot;, … &quot;Sun Jun 09 07:10:10 +0000 2013&quot;]</td><td>[-180, -180, … -180]</td></tr><tr><td>200312</td><td>31</td><td>41</td><td>[&quot;4ade0eeff964a5207e7021e3&quot;, &quot;4af2c3a6f964a52067e821e3&quot;, … &quot;4cd149843e63721ee0a9a2cc&quot;]</td><td>[&quot;Fri Oct 19 14:51:57 +0000 2012&quot;, &quot;Fri Oct 19 16:52:58 +0000 2012&quot;, … &quot;Sat Sep 14 15:44:27 +0000 2013&quot;]</td><td>[60, 60, … -240]</td></tr><tr><td>209485</td><td>28</td><td>29</td><td>[&quot;4c6a761f897b1b8d2dbeb117&quot;, &quot;4b0587a6f964a5203d9e22e3&quot;, … &quot;4b7e73c3f964a5209ced2fe3&quot;]</td><td>[&quot;Sat Apr 14 21:57:08 +0000 2012&quot;, &quot;Sat Apr 14 22:16:19 +0000 2012&quot;, … &quot;Sun Apr 29 14:11:12 +0000 2012&quot;]</td><td>[540, 540, … 540]</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>246290</td><td>31</td><td>39</td><td>[&quot;4a354c80f964a520c59c1fe3&quot;, &quot;4b159c3ef964a52016b123e3&quot;, … &quot;4b57b828f964a520a33d28e3&quot;]</td><td>[&quot;Thu Jul 05 18:47:36 +0000 2012&quot;, &quot;Fri Jul 06 18:45:10 +0000 2012&quot;, … &quot;Sat Mar 16 22:11:11 +0000 2013&quot;]</td><td>[-420, -420, … -420]</td></tr><tr><td>123605</td><td>26</td><td>44</td><td>[&quot;4cd9494e2a87a143dd90b109&quot;, &quot;4bf734cb4a67c9281a4f23cf&quot;, … &quot;4b155034f964a5201db023e3&quot;]</td><td>[&quot;Tue Apr 03 18:16:51 +0000 2012&quot;, &quot;Sat Apr 07 15:08:17 +0000 2012&quot;, … &quot;Sun Sep 09 13:57:18 +0000 2012&quot;]</td><td>[-300, -300, … -300]</td></tr><tr><td>246013</td><td>21</td><td>29</td><td>[&quot;4ae35581f964a520999321e3&quot;, &quot;4bc8e9e5cc8cd13adb93bacf&quot;, … &quot;4a5d7fa0f964a520a9bd1fe3&quot;]</td><td>[&quot;Sun May 27 23:30:46 +0000 2012&quot;, &quot;Wed May 30 01:53:48 +0000 2012&quot;, … &quot;Sun Aug 04 22:33:55 +0000 2013&quot;]</td><td>[-300, -300, … -360]</td></tr><tr><td>98164</td><td>32</td><td>37</td><td>[&quot;4e1c6e5f45dd94fe4a52c2ed&quot;, &quot;4da052d7784f3704a8bf96af&quot;, … &quot;4f6a6fe4e4b0d8154b452fbe&quot;]</td><td>[&quot;Wed May 23 17:57:42 +0000 2012&quot;, &quot;Wed May 23 18:16:45 +0000 2012&quot;, … &quot;Fri Jul 05 22:50:16 +0000 2013&quot;]</td><td>[-180, -180, … -180]</td></tr><tr><td>51894</td><td>33</td><td>42</td><td>[&quot;4b76f871f964a520e36f2ee3&quot;, &quot;4e5cddc145dd045aab533a23&quot;, … &quot;5106248ae4b040a89461909d&quot;]</td><td>[&quot;Sun Jul 08 07:13:26 +0000 2012&quot;, &quot;Sun Jul 15 14:07:35 +0000 2012&quot;, … &quot;Sat Aug 31 10:34:28 +0000 2013&quot;]</td><td>[180, 180, … 180]</td></tr></tbody></table></div>"],"text/plain":["shape: (21_697, 6)\n","┌────────┬────────┬────────────┬──────────────────────┬──────────────────────┬─────────────────────┐\n","│ user   ┆ n_pois ┆ n_checkins ┆ pois                 ┆ dates                ┆ TZs                 │\n","│ ---    ┆ ---    ┆ ---        ┆ ---                  ┆ ---                  ┆ ---                 │\n","│ i64    ┆ u32    ┆ u32        ┆ list[str]            ┆ list[str]            ┆ list[i64]           │\n","╞════════╪════════╪════════════╪══════════════════════╪══════════════════════╪═════════════════════╡\n","│ 81801  ┆ 18     ┆ 27         ┆ [\"4bac6e11f964a520c8 ┆ [\"Sun Jun 10         ┆ [120, 120, … 120]   │\n","│        ┆        ┆            ┆ f43ae3\", \"…          ┆ 04:15:00 +0000 20…   ┆                     │\n","│ 86919  ┆ 32     ┆ 38         ┆ [\"4b8567f2f964a520b8 ┆ [\"Mon Jul 02         ┆ [-240, -240, …      │\n","│        ┆        ┆            ┆ 5b31e3\", \"…          ┆ 19:19:07 +0000 20…   ┆ -240]               │\n","│ 85865  ┆ 24     ┆ 34         ┆ [\"4bb665b86edc76b001 ┆ [\"Sat May 05         ┆ [-180, -180, …      │\n","│        ┆        ┆            ┆ e0301c\", \"…          ┆ 14:35:08 +0000 20…   ┆ -180]               │\n","│ 200312 ┆ 31     ┆ 41         ┆ [\"4ade0eeff964a5207e ┆ [\"Fri Oct 19         ┆ [60, 60, … -240]    │\n","│        ┆        ┆            ┆ 7021e3\", \"…          ┆ 14:51:57 +0000 20…   ┆                     │\n","│ 209485 ┆ 28     ┆ 29         ┆ [\"4c6a761f897b1b8d2d ┆ [\"Sat Apr 14         ┆ [540, 540, … 540]   │\n","│        ┆        ┆            ┆ beb117\", \"…          ┆ 21:57:08 +0000 20…   ┆                     │\n","│ …      ┆ …      ┆ …          ┆ …                    ┆ …                    ┆ …                   │\n","│ 246290 ┆ 31     ┆ 39         ┆ [\"4a354c80f964a520c5 ┆ [\"Thu Jul 05         ┆ [-420, -420, …      │\n","│        ┆        ┆            ┆ 9c1fe3\", \"…          ┆ 18:47:36 +0000 20…   ┆ -420]               │\n","│ 123605 ┆ 26     ┆ 44         ┆ [\"4cd9494e2a87a143dd ┆ [\"Tue Apr 03         ┆ [-300, -300, …      │\n","│        ┆        ┆            ┆ 90b109\", \"…          ┆ 18:16:51 +0000 20…   ┆ -300]               │\n","│ 246013 ┆ 21     ┆ 29         ┆ [\"4ae35581f964a52099 ┆ [\"Sun May 27         ┆ [-300, -300, …      │\n","│        ┆        ┆            ┆ 9321e3\", \"…          ┆ 23:30:46 +0000 20…   ┆ -360]               │\n","│ 98164  ┆ 32     ┆ 37         ┆ [\"4e1c6e5f45dd94fe4a ┆ [\"Wed May 23         ┆ [-180, -180, …      │\n","│        ┆        ┆            ┆ 52c2ed\", \"…          ┆ 17:57:42 +0000 20…   ┆ -180]               │\n","│ 51894  ┆ 33     ┆ 42         ┆ [\"4b76f871f964a520e3 ┆ [\"Sun Jul 08         ┆ [180, 180, … 180]   │\n","│        ┆        ┆            ┆ 6f2ee3\", \"…          ┆ 07:13:26 +0000 20…   ┆                     │\n","└────────┴────────┴────────────┴──────────────────────┴──────────────────────┴─────────────────────┘"]},"execution_count":182,"metadata":{},"output_type":"execute_result"}],"source":["data_culled"]},{"cell_type":"code","execution_count":183,"metadata":{"id":"n5IdbHrNCCyp"},"outputs":[],"source":["data_culled = data_culled.with_columns(\n","    [\n","        rs.col(\"pois\")\n","        .list.eval(\n","            rs.element().is_in(frequent_pois),\n","        )\n","        .alias(\"is_frequent\")\n","    ]\n",")  # prep mask"]},{"cell_type":"code","execution_count":184,"metadata":{"id":"rpFQCNJGCCyq"},"outputs":[],"source":["final_data = (\n","    data_culled.lazy()\n","    .explode(\n","        [\n","            \"pois\",\n","            \"dates\",\n","            \"TZs\",\n","            \"is_frequent\",\n","        ]\n","    )\n","    .group_by(\"user\")\n","    .agg(\n","        [\n","            rs.col(\"pois\").filter(rs.col(\"is_frequent\")).alias(\"pois\"),\n","            rs.col(\"dates\").filter(rs.col(\"is_frequent\")).alias(\"dates\"),\n","            rs.col(\"TZs\").filter(rs.col(\"is_frequent\")).alias(\"TZs\"),\n","            rs.col(\"pois\").filter(rs.col(\"is_frequent\")).n_unique().alias(\"n_pois\"),\n","            rs.col(\"pois\").filter(rs.col(\"is_frequent\")).count().alias(\"n_checkins\"),\n","        ]\n","    )\n","    .filter(rs.col(\"n_checkins\") > 0)\n","    .filter(rs.col(\"n_pois\") > 0)\n","    .collect()\n",")  # filter out infrequent pois and users with no pois"]},{"cell_type":"code","execution_count":185,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"id":"CS0MglAhCCyq","outputId":"2d1e27dd-8820-4401-f408-990a711b0cea"},"outputs":[{"data":{"text/html":["<div><style>\n",".dataframe > thead > tr,\n",".dataframe > tbody > tr {\n","  text-align: right;\n","  white-space: pre-wrap;\n","}\n","</style>\n","<small>shape: (9, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>user</th><th>pois</th><th>dates</th><th>TZs</th><th>n_pois</th><th>n_checkins</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>19862.0</td><td>19862.0</td><td>19862.0</td><td>19862.0</td><td>19862.0</td><td>19862.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>156852.822274</td><td>null</td><td>null</td><td>null</td><td>6.123452</td><td>8.831437</td></tr><tr><td>&quot;std&quot;</td><td>76314.892884</td><td>null</td><td>null</td><td>null</td><td>4.609024</td><td>6.877662</td></tr><tr><td>&quot;min&quot;</td><td>49.0</td><td>null</td><td>null</td><td>null</td><td>1.0</td><td>1.0</td></tr><tr><td>&quot;25%&quot;</td><td>95613.0</td><td>null</td><td>null</td><td>null</td><td>3.0</td><td>4.0</td></tr><tr><td>&quot;50%&quot;</td><td>167846.0</td><td>null</td><td>null</td><td>null</td><td>5.0</td><td>7.0</td></tr><tr><td>&quot;75%&quot;</td><td>224576.0</td><td>null</td><td>null</td><td>null</td><td>8.0</td><td>12.0</td></tr><tr><td>&quot;max&quot;</td><td>266909.0</td><td>null</td><td>null</td><td>null</td><td>32.0</td><td>46.0</td></tr></tbody></table></div>"],"text/plain":["shape: (9, 7)\n","┌────────────┬───────────────┬─────────┬─────────┬─────────┬──────────┬────────────┐\n","│ statistic  ┆ user          ┆ pois    ┆ dates   ┆ TZs     ┆ n_pois   ┆ n_checkins │\n","│ ---        ┆ ---           ┆ ---     ┆ ---     ┆ ---     ┆ ---      ┆ ---        │\n","│ str        ┆ f64           ┆ f64     ┆ f64     ┆ f64     ┆ f64      ┆ f64        │\n","╞════════════╪═══════════════╪═════════╪═════════╪═════════╪══════════╪════════════╡\n","│ count      ┆ 19862.0       ┆ 19862.0 ┆ 19862.0 ┆ 19862.0 ┆ 19862.0  ┆ 19862.0    │\n","│ null_count ┆ 0.0           ┆ 0.0     ┆ 0.0     ┆ 0.0     ┆ 0.0      ┆ 0.0        │\n","│ mean       ┆ 156852.822274 ┆ null    ┆ null    ┆ null    ┆ 6.123452 ┆ 8.831437   │\n","│ std        ┆ 76314.892884  ┆ null    ┆ null    ┆ null    ┆ 4.609024 ┆ 6.877662   │\n","│ min        ┆ 49.0          ┆ null    ┆ null    ┆ null    ┆ 1.0      ┆ 1.0        │\n","│ 25%        ┆ 95613.0       ┆ null    ┆ null    ┆ null    ┆ 3.0      ┆ 4.0        │\n","│ 50%        ┆ 167846.0      ┆ null    ┆ null    ┆ null    ┆ 5.0      ┆ 7.0        │\n","│ 75%        ┆ 224576.0      ┆ null    ┆ null    ┆ null    ┆ 8.0      ┆ 12.0       │\n","│ max        ┆ 266909.0      ┆ null    ┆ null    ┆ null    ┆ 32.0     ┆ 46.0       │\n","└────────────┴───────────────┴─────────┴─────────┴─────────┴──────────┴────────────┘"]},"execution_count":185,"metadata":{},"output_type":"execute_result"}],"source":["final_data.describe()"]},{"cell_type":"markdown","metadata":{"id":"ItBEF5DFCCyq"},"source":["At this stage, culling is done, we can appreciate that `polars`'s SQL/functional-style API is different from Pandas, but it is very powerful and efficient."]},{"cell_type":"markdown","metadata":{"id":"HxS9E5bBCCyq"},"source":["The next step is geohashing the POIs, that is, we want to convert the latitude-longitude positions of the POIs into a grid-based geohash representation, which will form the basis for our network's embeddings."]},{"cell_type":"code","execution_count":186,"metadata":{"id":"5tSt9gYqCCyq"},"outputs":[],"source":["import geohash2 as gh\n","\n","POI_DATASET = \"/kaggle/input/dataset-tist2015/dataset_TIST2015_POIs.txt\"\n","\n","pois = rs.read_csv(\n","    POI_DATASET,\n","    has_header=False,\n","    low_memory=True,\n","    separator=\"\\t\",\n",")\n","pois.columns = [\"poi\", \"lat\", \"long\", \"category\", \"country\"]\n","pois = pois.drop(\"category\").drop(\"country\")"]},{"cell_type":"code","execution_count":187,"metadata":{"id":"fa3YSEGrCCyq"},"outputs":[],"source":["pois = (\n","    pois.lazy()\n","    .filter(rs.col(\"poi\").is_in(frequent_pois))\n","    .select(\n","        [\n","            rs.col(\"poi\"),\n","            rs.struct(\n","                [\n","                    rs.col(\"lat\").cast(rs.Float32),\n","                    rs.col(\"long\").cast(rs.Float32),\n","                ]\n","            )\n","            .alias(\"location\")\n","            .map_elements(\n","                lambda s: gh.encode(s[\"lat\"], s[\"long\"], precision=6),\n","                return_dtype=rs.String,\n","            )\n","            .alias(\"geohash\"),\n","        ]\n","    )\n","    .collect()\n",")\n","poi_geo_dict = dict(zip(pois[\"poi\"], pois[\"geohash\"]))"]},{"cell_type":"code","execution_count":188,"metadata":{"id":"y1GevNOtCCyq"},"outputs":[],"source":["# for each row in final_data, add the geohash of the pois by hitting the poi_geo_dict\n","\n","final_data = final_data.with_columns(\n","    [\n","        rs.col(\"pois\")\n","        .map_elements(\n","            lambda s: [poi_geo_dict[s] for s in s],\n","            return_dtype=rs.List(rs.String),\n","        )\n","        .alias(\"geohashes\")\n","    ]\n",")"]},{"cell_type":"code","execution_count":189,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qaXNXfVfCCyq","outputId":"5beacffe-7ca1-44f0-887b-4db6f26569f5"},"outputs":[{"data":{"text/plain":["['Fri Nov 09 22:50:56 +0000 2012',\n"," 'Fri Nov 09 23:22:45 +0000 2012',\n"," 'Fri Nov 23 20:39:54 +0000 2012',\n"," 'Sat Dec 01 23:42:20 +0000 2012',\n"," 'Fri Dec 28 23:11:51 +0000 2012',\n"," 'Sat Dec 29 13:13:18 +0000 2012',\n"," 'Sun Dec 30 23:48:31 +0000 2012',\n"," 'Mon Dec 31 22:07:14 +0000 2012',\n"," 'Sat Jan 12 01:20:31 +0000 2013',\n"," 'Sat Feb 02 10:49:07 +0000 2013',\n"," 'Sat Feb 02 13:21:20 +0000 2013',\n"," 'Wed Feb 13 15:59:32 +0000 2013',\n"," 'Wed Feb 13 16:58:17 +0000 2013',\n"," 'Sat Feb 16 19:16:14 +0000 2013',\n"," 'Sat Mar 09 21:20:30 +0000 2013',\n"," 'Sat Mar 09 23:56:13 +0000 2013',\n"," 'Sun Mar 17 22:50:24 +0000 2013',\n"," 'Wed Apr 03 16:23:58 +0000 2013',\n"," 'Wed Apr 03 16:24:16 +0000 2013',\n"," 'Thu Apr 11 22:17:29 +0000 2013',\n"," 'Thu Apr 25 19:54:46 +0000 2013',\n"," 'Sun Apr 28 15:58:24 +0000 2013',\n"," 'Sun Apr 28 18:12:51 +0000 2013',\n"," 'Sun Apr 28 22:40:46 +0000 2013',\n"," 'Wed May 01 22:07:09 +0000 2013',\n"," 'Wed May 01 22:13:54 +0000 2013',\n"," 'Tue May 07 19:11:52 +0000 2013',\n"," 'Fri Aug 09 13:26:03 +0000 2013',\n"," 'Sat Aug 31 19:43:02 +0000 2013',\n"," 'Sat Aug 31 20:29:42 +0000 2013']"]},"execution_count":189,"metadata":{},"output_type":"execute_result"}],"source":["final_data[\"dates\"][79].to_list()  # check out a temporal sequence"]},{"cell_type":"code","execution_count":190,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BenJc82OCCyq","outputId":"934d7234-d55c-4d0f-c322-47cfee96622b"},"outputs":[{"data":{"text/plain":["[120,\n"," 120,\n"," 120,\n"," 120,\n"," 120,\n"," 120,\n"," 120,\n"," 120,\n"," 120,\n"," 120,\n"," 120,\n"," 120,\n"," 120,\n"," 120,\n"," 120,\n"," 120,\n"," 120,\n"," 180,\n"," 180,\n"," 180,\n"," 180,\n"," 180,\n"," 180,\n"," 180,\n"," 180,\n"," 180,\n"," 180,\n"," 180,\n"," 180,\n"," 180]"]},"execution_count":190,"metadata":{},"output_type":"execute_result"}],"source":["final_data[\"TZs\"][79].to_list()  # ... and the corresponding timezones"]},{"cell_type":"markdown","metadata":{"id":"Vkv4I9FhCCyq"},"source":["The work *might* seem over, however, we still have timezones to account for, we want to normalize everything according to GMT, so we convert the timestamps accordingly."]},{"cell_type":"code","execution_count":191,"metadata":{"id":"ufyhRTJMCCyq"},"outputs":[],"source":["import datetime\n","\n","\n","def UTC_to_local(utc, tz):\n","\n","    date = datetime.datetime.strptime(utc, \"%a %b %d %H:%M:%S %z %Y\")\n","    date = date.replace(tzinfo=datetime.timezone.utc)\n","\n","    # shift by tz offset\n","    date = date.astimezone(datetime.timezone(datetime.timedelta(minutes=tz)))\n","\n","    date_s = datetime.datetime.strftime(date, \"%Y-%m-%d %H:%M:%S\")\n","    return date_s\n","\n","\n","def to_UNIX_time(date):\n","    return datetime.datetime.strptime(\n","        date, \"%Y-%m-%d %H:%M:%S\"\n","    ).timestamp()  # we use UNIX time as a key for sorting the POIs in our polars query"]},{"cell_type":"code","execution_count":192,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"P1n-YGXLCCyq","outputId":"0020709b-a7c8-4667-f68e-b0c7e26d2047"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2012-05-21 08:53:01'"]},"execution_count":192,"metadata":{},"output_type":"execute_result"}],"source":["UTC_to_local(\"Mon May 21 15:53:01 +0000 2012\", -420)  # example of usage"]},{"cell_type":"code","execution_count":193,"metadata":{"id":"-QJbyPkeCCyq"},"outputs":[],"source":["final_data = final_data.with_columns(\n","    [\n","        rs.struct([rs.col(\"dates\"), rs.col(\"TZs\")])\n","        .alias(\"times\")\n","        .map_elements(\n","            lambda struct: [\n","                UTC_to_local(date, tz)\n","                for date, tz in zip(struct[\"dates\"], struct[\"TZs\"])\n","            ],\n","            return_dtype=rs.List(rs.String),\n","        )\n","    ]\n",")  # This performs timezone conversion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mI9bmD3UCCyt"},"outputs":[],"source":["final_sorted = final_data.select(  # sort the times\n","    [\n","        rs.col(\"user\"),\n","        rs.struct(\n","            [\n","                rs.col(\"pois\"),\n","                rs.col(\"times\"),\n","            ]\n","        ).map_elements(\n","            lambda struct: [\n","                poi\n","                for poi, _ in sorted(\n","                    zip(  # here we sort the POIs struct by UNIX timestamps of the GMT times\n","                        struct[\"pois\"], [to_UNIX_time(date) for date in struct[\"times\"]]\n","                    ),\n","                    key=lambda s: s[1],\n","                )\n","            ],\n","            return_dtype=rs.List(rs.String),\n","        ),\n","        rs.struct(\n","            [\n","                rs.col(\"geohashes\"),\n","                rs.col(\"times\"),\n","            ]\n","        ).map_elements(\n","            lambda struct: [\n","                geo\n","                for geo, _ in sorted(\n","                    zip(\n","                        struct[\"geohashes\"],  # same thing goes on for geohashes\n","                        [to_UNIX_time(date) for date in struct[\"times\"]],\n","                    ),\n","                    key=lambda s: s[1],\n","                )\n","            ],\n","            return_dtype=rs.List(rs.String),\n","        ),\n","        rs.col(\"times\")\n","        .map_elements(\n","            lambda dates: sorted(dates, key=to_UNIX_time),\n","            return_dtype=rs.List(rs.String),\n","        )\n","        .alias(\"times_sorted\"),\n","        rs.col(\"n_checkins\"),\n","    ]\n",")\n","\n","# P.S, admittedly, it would have been more efficient to encode the geohashes *after* sorting the POIs,\n","# so that we could save on the sorting of the geohashes. Tough luck, you can't win 'em all."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8I9t4dirCCyt"},"outputs":[],"source":["final_sorted"]},{"cell_type":"markdown","metadata":{"id":"w-uFPvXPCCyt"},"source":["we now need to obtain a dataframe containing: each POI, it's geohash, and a set of all the check-ins it appears in\n","this is just one `polars` query away!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_nS1s8GECCyt"},"outputs":[],"source":["pois_checkins = final_sorted.explode([\"pois\", \"geohashes\"]).drop(\"n_checkins\")\n","\n","pois_checkins = (\n","    pois_checkins.with_columns(\n","        [\n","            rs.col(\"geohashes\").map_elements(lambda s: s[:4], rs.String).alias(\"g4\"),\n","        ]\n","    )\n","    .drop(\"geohashes\")\n","    .group_by([\"pois\", \"g4\"])\n","    .agg([rs.col(\"times_sorted\").flatten().alias(\"checkin_times\")])\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-2qN1sPCCyt"},"outputs":[],"source":["pois_checkins  # with this we can *efficiently* build our POI-POI spatial-temporal graphs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XY2TcDbXCCyt"},"outputs":[],"source":["def UTC_to_weekslot(utc: str) -> int:\n","    \"\"\"UTC_to_weekslot converts a UTC timestamp to a weekslot.\n","\n","    Parameters\n","    ----------\n","    utc : str\n","        A string representing a UTC timestamp.\n","\n","    Returns\n","    -------\n","    int\n","        A weekslot in the range [0, 56).\n","    \"\"\"\n","\n","    date = datetime.datetime.strptime(utc, \"%Y-%m-%d %H:%M:%S\")\n","    week = date.weekday()\n","    hour = date.hour\n","\n","    return week * 8 + hour // 3"]},{"cell_type":"markdown","metadata":{"id":"8fkRkLFgCCyt"},"source":["Next, we want to encode all of our inputs for our neural networks, this could *probably* be done\n","with polars magic, but it's too delicate and we prefer classic for-looping."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ItDfQyVUCCyt"},"outputs":[],"source":["encoder_dict = {\n","    \"users\": LabelEncoder(),\n","    \"pois\": LabelEncoder(),\n","    \"g2\": LabelEncoder(),\n","    \"g3\": LabelEncoder(),\n","    \"g4\": LabelEncoder(),\n","    \"g5\": LabelEncoder(),\n","    \"g6\": LabelEncoder(),\n","}\n","\n","encoded_data = {\n","    \"users\": [],\n","    \"pois\": [],\n","    \"g2\": [],\n","    \"g3\": [],\n","    \"g4\": [],\n","    \"g5\": [],\n","    \"g6\": [],\n","}\n","\n","unique_data = {\n","    \"users\": set(),\n","    \"pois\": set(),\n","    \"g2\": set(),\n","    \"g3\": set(),\n","    \"g4\": set(),\n","    \"g5\": set(),\n","    \"g6\": set(),\n","}\n","\n","# quick and dirty encoding:\n","# 1. put every unique symbol in a list\n","# 2. fit the respective encoder\n","# 3. transform the lists\n","\n","for i, row in enumerate(final_sorted.iter_rows()):\n","\n","    user, pois, geohashes, times_sorted, n_checkins = row\n","\n","    g2 = [geo[:2] for geo in geohashes]\n","    g3 = [geo[:3] for geo in geohashes]\n","    g4 = [geo[:4] for geo in geohashes]\n","    g5 = [geo[:5] for geo in geohashes]\n","    g6 = [geo[:6] for geo in geohashes]  # redundant, but I like symmetry\n","\n","    unique_data[\"users\"].add(user)\n","    unique_data[\"pois\"].update(pois)\n","    unique_data[\"g2\"].update(g2)\n","    unique_data[\"g3\"].update(g3)\n","    unique_data[\"g4\"].update(g4)\n","    unique_data[\"g5\"].update(g5)\n","    unique_data[\"g6\"].update(g6)\n","\n","for property, enc, data in zip(\n","    encoder_dict.keys(), encoder_dict.values(), unique_data.values()\n","):\n","    enc.fit(list(data))\n","    encoder_dict[property] = enc"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Il4AnsMUCCyt"},"outputs":[],"source":["# this could be optimized, right now it takes a while, at least we have a nice progress bar to look at\n","\n","ds_size = len(final_sorted)\n","\n","for i, row in tqdm(enumerate(final_sorted.iter_rows()), total=ds_size):\n","\n","    user, pois, geohashes, times_sorted, n_checkins = row\n","\n","    g2 = [geo[:2] for geo in geohashes]\n","    g3 = [geo[:3] for geo in geohashes]\n","    g4 = [geo[:4] for geo in geohashes]\n","    g5 = [geo[:5] for geo in geohashes]\n","    g6 = [geo[:6] for geo in geohashes]\n","\n","    encoded_data[\"users\"].append(encoder_dict[\"users\"].transform([user])[0])\n","    encoded_data[\"pois\"].append(encoder_dict[\"pois\"].transform(pois))\n","    encoded_data[\"g2\"].append(encoder_dict[\"g2\"].transform(g2))\n","    encoded_data[\"g3\"].append(encoder_dict[\"g3\"].transform(g3))\n","    encoded_data[\"g4\"].append(encoder_dict[\"g4\"].transform(g4))\n","    encoded_data[\"g5\"].append(encoder_dict[\"g5\"].transform(g5))\n","    encoded_data[\"g6\"].append(encoder_dict[\"g6\"].transform(g6))\n","\n","    # sum 1 to all values to avoid 0s\n","    encoded_data[\"users\"][-1] += 1\n","    encoded_data[\"pois\"][-1] += 1\n","    encoded_data[\"g2\"][-1] += 1\n","    encoded_data[\"g3\"][-1] += 1\n","    encoded_data[\"g4\"][-1] += 1\n","    encoded_data[\"g5\"][-1] += 1\n","    encoded_data[\"g6\"][-1] += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nN6GpNCFCCyu"},"outputs":[],"source":["# check that we left space for the padding token\n","min((arr.min() for arr in encoded_data[\"pois\"]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"enab_ljUCCyu"},"outputs":[],"source":["pois_checkins"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qyEXUwAcCCyu"},"outputs":[],"source":["# we also encode the graph dataframe so we can build the graphs\n","\n","pois_checkins = (\n","    pois_checkins.lazy()\n","    .with_columns(\n","        [\n","            rs.col(\"pois\").map_elements(\n","                lambda s: encoder_dict[\"pois\"].transform([s])[0] + 1, rs.Int64\n","            ),\n","            rs.col(\"g4\").map_elements(\n","                lambda s: encoder_dict[\"g4\"].transform([s])[0] + 1, rs.Int64\n","            ),  # apply utc_to_weekslot to each timestamp in the list\n","            rs.col(\"checkin_times\").map_elements(\n","                lambda s: [UTC_to_weekslot(date) for date in s], rs.List(rs.Int64)\n","            ),\n","        ]\n","    )\n","    .sort(\"pois\")\n","    .collect()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"w1-P4fDgCCyu"},"outputs":[],"source":["# add fictitious POI 0 to the graph, with nonexistent geohash and no timeslot, so we get a 0 row and column for the padding token\n","fake_datapoint = rs.DataFrame(\n","    {\n","        \"pois\": [0],\n","        \"g4\": [pois_checkins[\"g4\"].max() + 42],\n","        \"checkin_times\": [[43]],\n","    }\n",")\n","# this is a lot of work since polars dataframes are immutable by default, we have to run a query to change the 43 into an empty list\n","# we NEED the 43 otherwise polars won't infer the datatype of the list\n","\n","fake_datapoint = fake_datapoint.with_columns(\n","    [rs.col(\"checkin_times\").map_elements(lambda s: [], rs.List(rs.Int64))]\n",")\n","\n","pois_checkins = fake_datapoint.vstack(pois_checkins)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"i4zaRXGPCCyu"},"outputs":[],"source":["spatial_row = np.array(pois_checkins[\"g4\"].to_list()).reshape(-1, 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rAL2xYaSCCyu"},"outputs":[],"source":["# outer product using equality\n","spatial_graph = (spatial_row == spatial_row.T).astype(np.int32)\n","spatial_graph[0, 0] = (\n","    0  # the fake g4 is still equal to itself, we suppress this equality\n",")\n","spatial_graph = torch.tensor(spatial_graph)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"O5Glb5TaCCyu"},"outputs":[],"source":["temporal_row = pois_checkins[\"checkin_times\"].to_list()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"EKaTa7VNCCyu"},"outputs":[],"source":["temporal_graph = np.zeros((spatial_row.shape[0], spatial_row.shape[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sWGavlW5CCyu"},"outputs":[],"source":["temporal_sets = [np.array(list(set(row))) for row in temporal_row]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IAWEvt1mCCyu"},"outputs":[],"source":["time_sets = torch.zeros((len(temporal_sets), 56), dtype=torch.int8)\n","\n","for i, r in enumerate(temporal_row):\n","    indices = torch.tensor(r, dtype=torch.long)\n","    time_sets[i, indices] = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wGjJgRWaCCyu"},"outputs":[],"source":["time_sets.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Ym-XXavSCCyu"},"outputs":[],"source":["# AND outer product\n","\n","intersection = time_sets @ time_sets.T\n","union = time_sets.unsqueeze(1) | time_sets.unsqueeze(0)\n","union = union.sum(dim=2)\n","iou = intersection / union"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"uXE6xSDoCCyu"},"outputs":[],"source":["temporal_graph = iou >= 0.9\n","# cast to int\n","temporal_graph = temporal_graph.int()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"F_e6VXbgCCyu"},"outputs":[],"source":["temporal_graph[0, :].sum()"]},{"cell_type":"markdown","metadata":{"id":"NeCUaMGtCCyu"},"source":["We print information about the sparsity of the graphs, we note that\n","the sparsity of the graphs is similar to that of the paper."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-xX2oUeoCCyu"},"outputs":[],"source":["temporal_density = (\n","    temporal_graph.sum() / (temporal_graph.shape[0] * temporal_graph.shape[1])\n",").item()\n","spatial_density = (\n","    spatial_graph.sum() / (spatial_graph.shape[0] * spatial_graph.shape[1])\n",").item()\n","\n","print(f\"Temporal sparsity: {(1 - temporal_density) * 100:.2f}%\")\n","\n","print(f\"Spatial sparsity: {(1 - spatial_density) * 100:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"6iiU7c42aItw"},"source":["## Train Test Split\n","\n","We now generate two dataframes from the `encoded_data` dataframe, one for training and one for testing.\n","\n","First, we have to drop every sequence that has less than 4 timestamps, as we wouldn't be able to get the minimum of two samples for each of the sets,\n","we then calculate the 80% of the sequences and split the data accordingly."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nNlKXjXbaItw"},"outputs":[],"source":["len(encoded_data[\"pois\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_tyiaen0aItw"},"outputs":[],"source":["total_data = rs.DataFrame(encoded_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"K67_I3DwaItw"},"outputs":[],"source":["total_data = total_data.with_columns(\n","    [\n","        rs.col(\"pois\").list.len().alias(\"length\"),\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Vzaok7ZSaItw"},"outputs":[],"source":["total_data = total_data.with_columns(\n","    rs.col(\"length\")\n","    .map_elements(lambda s: int(0.8 * s) - 1, rs.Int64)\n","    .alias(\"train_end\")\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hNmcZlX2aItw"},"outputs":[],"source":["# drop sequences that are too short\n","total_data = total_data.filter(\n","    (\n","        rs.col(\"train_end\") >= 1\n","    )  # at least 2 elements in the training set (1 is the index)\n","    & (\n","        rs.col(\"length\") - (rs.col(\"train_end\") + 1) >= 2\n","    )  # at least 2 elements in the validation set\n",")\n","print(total_data[\"length\"].mean())\n","print(total_data.count())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Mbl9-lbgaItx"},"outputs":[],"source":["total_data.sort(\"length\")  # check out the distribution of sequence lengths"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"-fdFTxtRaItx"},"outputs":[],"source":["# Check if the shortest sequence is long enough\n","total_data.sort(\"length\")[\"pois\"][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"bjuQpQN2aItx"},"outputs":[],"source":["# slice the two dataframes\n","train_data = total_data.select(\n","    [\n","        rs.col(\"users\"),\n","        rs.struct(\n","            [\n","                rs.col(\"pois\"),\n","                rs.col(\"g2\"),\n","                rs.col(\"g3\"),\n","                rs.col(\"g4\"),\n","                rs.col(\"g5\"),\n","                rs.col(\"g6\"),\n","                rs.col(\"train_end\"),\n","            ]\n","        )\n","        .map_elements(\n","            lambda struct: [\n","                struct[\"pois\"][: struct[\"train_end\"]],\n","                struct[\"g2\"][: struct[\"train_end\"]],\n","                struct[\"g3\"][: struct[\"train_end\"]],\n","                struct[\"g4\"][: struct[\"train_end\"]],\n","                struct[\"g5\"][: struct[\"train_end\"]],\n","                struct[\"g6\"][: struct[\"train_end\"]],\n","            ],\n","            return_dtype=rs.List(rs.List(rs.Int64)),\n","        )\n","        .alias(\"sequences\"),\n","    ]\n",")\n","\n","\n","test_data = total_data.select(\n","    [\n","        rs.col(\"users\"),\n","        rs.struct(\n","            [\n","                rs.col(\"pois\"),\n","                rs.col(\"g2\"),\n","                rs.col(\"g3\"),\n","                rs.col(\"g4\"),\n","                rs.col(\"g5\"),\n","                rs.col(\"g6\"),\n","                rs.col(\"train_end\"),\n","            ]\n","        )\n","        .map_elements(\n","            lambda struct: [\n","                struct[\"pois\"][struct[\"train_end\"] :],\n","                struct[\"g2\"][struct[\"train_end\"] :],\n","                struct[\"g3\"][struct[\"train_end\"] :],\n","                struct[\"g4\"][struct[\"train_end\"] :],\n","                struct[\"g5\"][struct[\"train_end\"] :],\n","                struct[\"g6\"][struct[\"train_end\"] :],\n","            ],\n","            return_dtype=rs.List(rs.List(rs.Int64)),\n","        )\n","        .alias(\"sequences\"),\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"zuqr6FDiaItx"},"outputs":[],"source":["def explode_dict(d):\n","    \"\"\"explode_dict Convert packed polars dataframe into a neat python dict\n","\n","    Parameters\n","    ----------\n","    d : Polars.DataFrame\n","        A polars dataframe with a struct column\n","\n","    Returns\n","    -------\n","    dict\n","        A python dict with the same structure as the struct column\n","    \"\"\"\n","    ret = {\n","        \"users\": d[\"users\"].to_list(),\n","        \"pois\": [],\n","        \"g2\": [],\n","        \"g3\": [],\n","        \"g4\": [],\n","        \"g5\": [],\n","        \"g6\": [],\n","    }\n","\n","    for sample in d[\"sequences\"]:\n","        pois, g2, g3, g4, g5, g6 = sample\n","        ret[\"pois\"].append(pois.to_list())\n","        ret[\"g2\"].append(g2.to_list())\n","        ret[\"g3\"].append(g3.to_list())\n","        ret[\"g4\"].append(g4.to_list())\n","        ret[\"g5\"].append(g5.to_list())\n","        ret[\"g6\"].append(g6.to_list())\n","\n","    return ret"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HJYSRKeJaItx"},"outputs":[],"source":["encoded_data_train = explode_dict(train_data.to_dict())\n","encoded_data_test = explode_dict(test_data.to_dict())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6TsVcLgXZPM0"},"outputs":[],"source":["total_data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"31OjFgF3ZPM0"},"outputs":[],"source":["encoded_data_train[\"pois\"][10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lPdKaRW-ZPM0"},"outputs":[],"source":["encoded_data_test[\"pois\"][10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9KbV5-4TZPM0"},"outputs":[],"source":["total_data[\"pois\"][10].to_list()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"6S2zqagAZPM0"},"outputs":[],"source":["min([len(l) for l in encoded_data_test[\"pois\"]])"]},{"cell_type":"markdown","metadata":{"id":"-PEXknqyCCyu"},"source":["## Metrics\n"]},{"cell_type":"markdown","metadata":{"id":"KxUt4jMGCCyu"},"source":["The paper utilizes metrics that check if the target is in the top-k recommendations, we implement them here."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4lW2sibnCCyv"},"outputs":[],"source":["class AccuracyAtK(nn.Module):\n","    def __init__(self, k: int):\n","        \"\"\"__init__ initializes the AccuracyAtK module.\n","\n","        Accuracy@k is the proportion of correct predictions in the top-k elements.\n","\n","        Parameters\n","        ----------\n","        k : int\n","            The number of top-k elements to consider.\n","\n","        \"\"\"\n","        super().__init__()\n","        self.k = k\n","\n","    def forward(\n","        self, logits: torch.Tensor, targets: torch.Tensor, padding_mask: torch.Tensor\n","    ) -> torch.Tensor:\n","        \"\"\"forward computes the accuracy at k between logits and targets.\n","\n","        Parameters\n","        ----------\n","        logits : torch.Tensor\n","            Class probability, either (B, C) or (B, T, C)\n","        targets : torch.Tensor\n","            Ground truth class indices, either (B,) or (B, T)\n","        padding_mask : torch.Tensor\n","            Padding mask, either (B,) or (B, T)\n","\n","        Returns\n","        -------\n","        torch.Tensor\n","            The accuracy at k, a scalar-tensor.\n","        \"\"\"\n","\n","        \n","        predicted=logits.softmax(dim=-1)\n","        top_k=predicted.topk(self.k, dim=-1)[1]\n","        correct=(top_k==targets.unsqueeze(-1)).any(dim=-1).float()\n","        if padding_mask is not None:\n","            correct *= padding_mask.float()\n","            # Avoid division by zero by counting non-zero elements in the mask\n","            accuracy = correct.sum() / padding_mask.float().sum()\n","        else:\n","            accuracy = correct.mean()\n","        print(\"accuracy\",accuracy)\n","\n","        return accuracy\n","        '''\n","        \n","        # Gotta have at least one nasty python one-liner, in memory of the old\n","        # programming lab 1 bachelor course\n","        \n","        # P.S the one liner was bugged, the hubris of man...\n","        return (\n","            (\n","                logits.softmax(dim=-1)  # apply softmax\n","                .masked_fill(\n","                    padding_mask.unsqueeze(-1), -1e9\n","                )  # mask padding by imposing a very low probability (hacky)\n","                .topk(self.k, dim=-1)[1]  # extract top-k indices\n","                == targets.unsqueeze(-1)\n","            )\n","            .any(dim=-1)\n","            .float()\n","            .mean()\n","        )\n","        '''\n","\n","\n","class AccuracyAt1(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n","\n","        predicted = logits.argmax(dim=-1)\n","        correct = (predicted == targets).float()\n","        accuracy = correct.mean()\n","        return accuracy\n","\n","\n","class MeanReciprocalRank(nn.Module):\n","\n","    def __init__(self):\n","        \"\"\"__init__ initializes the MeanReciprocalRank module.\n","\n","        Mean reciprocal rank is the average of the reciprocal ranks of the top-k elements.\n","\n","        \"\"\"\n","        super().__init__()\n","\n","    def forward(\n","        self, logits: torch.Tensor, targets: torch.Tensor, padding_mask: torch.Tensor\n","    ) -> torch.Tensor:\n","        \"\"\"forward computes the mean reciprocal rank between logits and targets.\n","\n","        Parameters\n","        ----------\n","        logits : torch.Tensor\n","            Class probability\n","        targets : torch.Tensor\n","            Ground truth class indices\n","        padding_mask : torch.Tensor\n","            Padding mask\n","\n","        Returns\n","        -------\n","        torch.Tensor\n","            The mean reciprocal rank, a scalar-tensor.\n","        \"\"\"\n","        \n","        predicted = logits.softmax(dim=-1)\n","        top_k = predicted.topk(logits.size(-1), dim=-1)[1]\n","        ranks = (top_k == targets.unsqueeze(-1)).nonzero()[:, -1].float() + 1\n","        reciprocal_ranks = 1.0 / ranks\n","        if padding_mask is not None:\n","            reciprocal_ranks *= padding_mask.float()\n","            # Avoid division by zero by counting non-zero elements in the mask\n","            mrr = reciprocal_ranks.sum() / padding_mask.float().sum()\n","        else:\n","            mrr = reciprocal_ranks.mean()\n","        return mrr"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"J0Q9II4HCCyv"},"outputs":[],"source":["from torch.utils.data import Dataset\n","from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n","\n","\n","def rnn_collation_fn(batch):\n","\n","    users = []\n","    pois = []\n","    g2 = []\n","    g3 = []\n","    g4 = []\n","    g5 = []\n","    g6 = []\n","\n","\n","    for user, poi, geo2, geo3, geo4, geo5, geo6 in batch:\n","        users.append(user)\n","        pois.append(poi)\n","        g2.append(geo2)\n","        g3.append(geo3)\n","        g4.append(geo4)\n","        g5.append(geo5)\n","        g6.append(geo6)\n","\n","\n","    seq = (\n","        torch.tensor(users, dtype=torch.long),\n","        pad_sequence(pois, batch_first=True, padding_value=0),\n","        pad_sequence(g2, batch_first=True, padding_value=0),\n","        pad_sequence(g3, batch_first=True, padding_value=0),\n","        pad_sequence(g4, batch_first=True, padding_value=0),\n","        pad_sequence(g5, batch_first=True, padding_value=0),\n","        pad_sequence(g6, batch_first=True, padding_value=0),\n","    )  # build a sequence\n","\n","    x = (\n","        seq[0],\n","        seq[1][:, :-1],\n","        seq[2][:, :-1],\n","        seq[3][:, :-1],\n","        seq[4][:, :-1],\n","        seq[5][:, :-1],\n","        seq[6][:, :-1],\n","    )  # omit the last one for sample\n","\n","    y = (\n","        seq[0],\n","        seq[1][:, 1:],\n","        seq[2][:, 1:],\n","        seq[3][:, 1:],\n","        seq[4][:, 1:],\n","        seq[5][:, 1:],\n","        seq[6][:, 1:],\n","    )  # omit the first one for ground truth\n","\n","    # Take sequence lengths\n","    x_lengths = x[1].count_nonzero(dim=1)\n","    x_lengths = x_lengths.tolist()\n","\n","\n","    return x,y,x_lengths\n","\n","\n","class CheckinDataset(Dataset):\n","    def __init__(self, data):\n","        self.data = data\n","\n","    def __len__(self):\n","        return len(self.data[\"users\"])\n","\n","    def __getitem__(self, idx):\n","\n","        x = (\n","            torch.tensor(self.data[\"users\"][idx], dtype=torch.long),\n","            torch.tensor(self.data[\"pois\"][idx], dtype=torch.long),\n","            torch.tensor(self.data[\"g2\"][idx], dtype=torch.long),\n","            torch.tensor(self.data[\"g3\"][idx], dtype=torch.long),\n","            torch.tensor(self.data[\"g4\"][idx], dtype=torch.long),\n","            torch.tensor(self.data[\"g5\"][idx], dtype=torch.long),\n","            torch.tensor(self.data[\"g6\"][idx], dtype=torch.long),\n","        )\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"y5gC9yrsCCyv"},"source":["## Dataset and Datamodule\n","\n","We then define a pytorch dataset and a custom collation function that allows us to dynamically\n","pad sequences to the longest one in the batch (as opposed to the longest one in the dataset)\n","as they are loaded during training, this gives us an edge in performance by dramatically reducing the\n","sparsity of our inputs."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ncOgD2VxCCyv"},"outputs":[],"source":["class CheckinModule(pl.LightningDataModule):\n","    def __init__(self, encoded_data_train, encoded_data_test, batch_size=32, workers=4):\n","        \"\"\"__init__ initializes the CheckinModule.\n","\n","        Parameters\n","        ----------\n","        encoded_data_train : Union[dict, rs.DataFrame]\n","            The training data.\n","        encoded_data_test : Union[dict, rs.DataFrame]\n","            The testing data.\n","        batch_size : int, optional\n","            Size of the batches, by default 32\n","        workers : int, optional\n","            Number of worker processes, by default 4\n","        \"\"\"\n","        super().__init__()\n","        self.encoded_data_train = encoded_data_train\n","        self.encoded_data_test = encoded_data_test\n","        self.batch_size = batch_size\n","        self.workers = workers\n","\n","        assert isinstance(self.encoded_data_train, dict) or isinstance(\n","            self.encoded_data_train, rs.DataFrame\n","        ), \"encoded_data_train must be a dict or a polars DataFrame\"\n","        assert isinstance(self.encoded_data_test, dict) or isinstance(\n","            self.encoded_data_test, rs.DataFrame\n","        ), \"encoded_data_test must be a dict or a polars DataFrame\"\n","\n","        assert batch_size > 0, \"batch_size must be a positive integer\"\n","        assert workers >= 0, \"workers must be a non-negative integer\"\n","\n","    def setup(self, stage=None):\n","\n","        if (\n","            isinstance(self.encoded_data_train, dict)\n","            or isinstance(self.encoded_data_train, rs.DataFrame)\n","        ) and (\n","            isinstance(self.encoded_data_test, dict)\n","            or isinstance(self.encoded_data_test, rs.DataFrame)\n","        ):\n","            print(\"Loading data from dict/dataframe\")\n","            self.train_dataset = CheckinDataset(self.encoded_data_train)\n","            self.test_dataset = CheckinDataset(self.encoded_data_test)\n","\n","        elif isinstance(self.encoded_data_train, CheckinDataset) and isinstance(\n","            self.encoded_data_test, CheckinDataset\n","        ):\n","            print(\"Loading data from pre-instantiated datasets\")\n","            self.train_dataset = self.encoded_data_train\n","            self.test_dataset = self.encoded_data_test\n","        else:\n","            raise ValueError(\"Invalid data type\")\n","\n","    def train_dataloader(self):\n","        return torch.utils.data.DataLoader(\n","            self.train_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=True,\n","            num_workers=self.workers,\n","            collate_fn=rnn_collation_fn,\n","        )\n","\n","    def val_dataloader(self):\n","        return torch.utils.data.DataLoader(\n","            self.test_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=False,\n","            num_workers=self.workers,\n","            collate_fn=rnn_collation_fn,\n","        )\n","\n","    def test_dataloader(self):\n","        return torch.utils.data.DataLoader(\n","            self.test_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=False,\n","            num_workers=self.workers,\n","            collate_fn=rnn_collation_fn,\n","        )\n","\n","    def save(self, whole_path, train_path, test_path):\n","\n","        torch.save(self.train_dataset, train_path)\n","        torch.save(self.test_dataset, test_path)\n","\n","    @staticmethod  # load without instantiating\n","    def load(train_path, test_path):\n","\n","        train_dataset = torch.load(train_path)\n","        test_dataset = torch.load(test_path)\n","\n","        return CheckinModule(train_dataset, test_dataset)"]},{"cell_type":"markdown","metadata":{"id":"G_STfUGVCCyv"},"source":["## Baseline model: LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"RcjMtxnFCCyv"},"outputs":[],"source":["@dataclass\n","class BaselineDimensions:\n","    nuser: int\n","    npoi: int\n","    g2len: int\n","    g3len: int\n","    g4len: int\n","    g5len: int\n","    g6len: int\n","\n","\n","# HMT_RN (Hierarchical Multi-Task Recurrent Network)\n","class HMT_RN(pl.LightningModule):\n","    def __init__(\n","        self,\n","        dimensions: BaselineDimensions,\n","        embedding_dim,\n","        lstm_hidden_dim,\n","        dropout_rate=0.9,\n","        # 0.9 is a lot, but the paper says so.\n","    ):\n","        super(HMT_RN, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = lstm_hidden_dim\n","        self.dims = dimensions\n","\n","        # Embedding layers one for user, one for poi and one for each G@P\n","        self.user_embedding = nn.Embedding(\n","            dimensions.nuser, embedding_dim, padding_idx=0\n","        )\n","        self.poi_embedding = nn.Embedding(dimensions.npoi, embedding_dim, padding_idx=0)\n","        self.g2_embed = nn.Embedding(dimensions.g2len, embedding_dim, padding_idx=0)\n","        self.g3_embed = nn.Embedding(dimensions.g3len, embedding_dim, padding_idx=0)\n","        self.g4_embed = nn.Embedding(dimensions.g4len, embedding_dim, padding_idx=0)\n","        self.g5_embed = nn.Embedding(dimensions.g5len, embedding_dim, padding_idx=0)\n","        self.g6_embed = nn.Embedding(dimensions.g6len, embedding_dim, padding_idx=0)\n","\n","        # Dropout layer for embeddings\n","        self.e_drop = nn.Dropout(p=dropout_rate)\n","\n","        # LSTM layer\n","        self.lstm = nn.LSTM(\n","            input_size=embedding_dim, hidden_size=lstm_hidden_dim, batch_first=True\n","        )\n","\n","        # Linear layers for prediction tasks\n","        self.linear_poi = nn.Linear(lstm_hidden_dim + embedding_dim, dimensions.npoi)\n","        self.linear_g2 = nn.Linear(lstm_hidden_dim + embedding_dim, dimensions.g2len)\n","        self.linear_g3 = nn.Linear(lstm_hidden_dim + embedding_dim, dimensions.g3len)\n","        self.linear_g4 = nn.Linear(lstm_hidden_dim + embedding_dim, dimensions.g4len)\n","        self.linear_g5 = nn.Linear(lstm_hidden_dim + embedding_dim, dimensions.g5len)\n","        self.linear_g6 = nn.Linear(lstm_hidden_dim + embedding_dim, dimensions.g6len)\n","\n","        # https://discuss.pytorch.org/t/ignore-padding-area-in-loss-computation/95804/6\n","        self.criterion = nn.CrossEntropyLoss(reduction=\"none\")\n","\n","        self.top1 = AccuracyAtK(1)\n","        self.top5 = AccuracyAtK(5)\n","        self.top10 = AccuracyAtK(10)\n","        self.top20 = AccuracyAtK(20)\n","        self.mrr = MeanReciprocalRank()\n","\n","        self.apply(self.init_weights)\n","\n","    def init_weights(self, w):\n","\n","        if type(w) == nn.Linear:\n","            nn.init.kaiming_normal_(w.weight)\n","            nn.init.constant_(w.bias, 0)\n","        elif type(w) == nn.LSTM:\n","            for name, param in w.named_parameters():\n","                if \"bias\" in name:\n","                    nn.init.constant_(param, 0)\n","                elif \"weight\" in name:\n","                    nn.init.kaiming_normal_(param)\n","        elif type(w) == nn.Embedding:\n","            nn.init.kaiming_normal_(w.weight)\n","            nn.init.constant_(w.weight[0], 0)\n","\n","    def forward(self, batch, lengths):\n","        \"\"\"forward passes the batch through the model.\n","\n","        Parameters\n","        ----------\n","        batch : `tuple[torch.Tensor]`\n","            A tuple of tensors ordered as follows:\n","            (users, poi, x_geoHash2, x_geoHash3, x_geoHash4, x_geoHash5, x_geoHash6)\n","        \"\"\"\n","\n","        users, poi, x_geoHash2, x_geoHash3, x_geoHash4, x_geoHash5, x_geoHash6 = batch\n","\n","        # unpack the packed sequences, retrieve the lengths, for the LSTM len_g2, etc... are ignored\n","#         poi, len_poi = pad_packed_sequence(poi, batch_first=True)\n","#         x_geoHash2, len_g2 = pad_packed_sequence(x_geoHash2, batch_first=True)\n","#         x_geoHash3, len_g3 = pad_packed_sequence(x_geoHash3, batch_first=True)\n","#         x_geoHash4, len_g4 = pad_packed_sequence(x_geoHash4, batch_first=True)\n","#         x_geoHash5, len_g5 = pad_packed_sequence(x_geoHash5, batch_first=True)\n","#         x_geoHash6, len_g6 = pad_packed_sequence(x_geoHash6, batch_first=True)\n","\n","        B, T = poi.shape\n","\n","        # make it so  that users are tiled T times\n","        users = users.repeat(T, 1).T\n","\n","        e_user = self.e_drop(self.user_embedding(users))\n","        e_poi = self.e_drop(self.poi_embedding(poi))\n","        e_gap2 = self.e_drop(self.g2_embed(x_geoHash2))\n","        e_gap3 = self.e_drop(self.g3_embed(x_geoHash3))\n","        e_gap4 = self.e_drop(self.g4_embed(x_geoHash4))\n","        e_gap5 = self.e_drop(self.g5_embed(x_geoHash5))\n","        e_gap6 = self.e_drop(self.g6_embed(x_geoHash6))\n","\n","        packed_poi = pack_padded_sequence(\n","            e_poi, lengths, batch_first=True, enforce_sorted=False\n","        )\n","        packed_output, (h_n, c_n) = self.lstm(packed_poi)\n","        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n","\n","        # dense layers\n","        next_poi = self.linear_poi(torch.cat((output, e_user), dim=2))\n","        next_g2 = self.linear_g2(torch.cat((output, e_gap2), dim=2))\n","        next_g3 = self.linear_g3(torch.cat((output, e_gap3), dim=2))\n","        next_g4 = self.linear_g4(torch.cat((output, e_gap4), dim=2))\n","        next_g5 = self.linear_g5(torch.cat((output, e_gap5), dim=2))\n","        next_g6 = self.linear_g6(torch.cat((output, e_gap6), dim=2))\n","\n","        return next_poi, next_g2, next_g3, next_g4, next_g5, next_g6\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y, lenpoi = batch\n","\n","        (\n","            poi_pred,\n","            gap2_pred,\n","            gap3_pred,\n","            gap4_pred,\n","            gap5_pred,\n","            gap6_pred,\n","        ) = self(x, lenpoi)\n","\n","        loss_mask = (y[1] != 0).reshape(-1)\n","\n","        loss_poi = (self.criterion(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap2 = (self.criterion(\n","            gap2_pred.reshape(-1, self.dims.g2len), y[2].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap3 = (self.criterion(\n","            gap3_pred.reshape(-1, self.dims.g3len), y[3].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap4 = (self.criterion(\n","            gap4_pred.reshape(-1, self.dims.g4len), y[4].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap5 = (self.criterion(\n","            gap5_pred.reshape(-1, self.dims.g5len), y[5].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap6 = (self.criterion(\n","            gap6_pred.reshape(-1, self.dims.g6len), y[6].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","\n","        loss = (\n","            loss_poi + loss_gap2 + loss_gap3 + loss_gap4 + loss_gap5 + loss_gap6\n","        ) / (loss_mask.sum() * 6)\n","        self.log(\"train/loss\", loss)\n","        self.log(\"train/loss_gap2\", loss_gap2)\n","        self.log(\"train/loss_gap3\", loss_gap3)\n","        self.log(\"train/loss_gap4\", loss_gap4)\n","        self.log(\"train/loss_gap5\", loss_gap5)\n","        self.log(\"train/loss_gap6\", loss_gap6)\n","        self.log(\"train/loss_poi\", loss_poi)\n","\n","        return {\"loss\": loss}\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y, lenpoi = batch\n","        (\n","            poi_pred,\n","            gap2_pred,\n","            gap3_pred,\n","            gap4_pred,\n","            gap5_pred,\n","            gap6_pred,\n","        ) = self(x, lenpoi)\n","\n","        loss_mask = (y[1] != 0).reshape(-1)\n","\n","        loss_poi = (self.criterion(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap2 = (self.criterion(\n","            gap2_pred.reshape(-1, self.dims.g2len), y[2].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap3 = (self.criterion(\n","            gap3_pred.reshape(-1, self.dims.g3len), y[3].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap4 = (self.criterion(\n","            gap4_pred.reshape(-1, self.dims.g4len), y[4].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap5 = (self.criterion(\n","            gap5_pred.reshape(-1, self.dims.g5len), y[5].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap6 = (self.criterion(\n","            gap6_pred.reshape(-1, self.dims.g6len), y[6].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","\n","        loss = (\n","            loss_poi + loss_gap2 + loss_gap3 + loss_gap4 + loss_gap5 + loss_gap6\n","        ) / (loss_mask.sum() * 6)\n","\n","\n","        top1_acc = self.top1(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","        top5_acc = self.top5(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","        top10_acc = self.top10(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","        top20_acc = self.top20(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","        mrr = self.mrr(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","\n","        self.log(\"val/loss\", loss)\n","        self.log(\"val/loss_gap2\", loss_gap2)\n","        self.log(\"val/loss_gap3\", loss_gap3)\n","        self.log(\"val/loss_gap4\", loss_gap4)\n","        self.log(\"val/loss_gap5\", loss_gap5)\n","        self.log(\"val/loss_gap6\", loss_gap6)\n","        self.log(\"val/loss_poi\", loss_poi)\n","\n","        # log \"leaderboard\" metrics\n","\n","        self.log(\"val/top1\", top1_acc)\n","        self.log(\"val/top5\", top5_acc)\n","        self.log(\"val/top10\", top10_acc)\n","        self.log(\"val/top20\", top20_acc)\n","        self.log(\"val/mrr\", mrr)\n","\n","        return loss\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y, lenpoi = batch\n","        (\n","            poi_pred,\n","            gap2_pred,\n","            gap3_pred,\n","            gap4_pred,\n","            gap5_pred,\n","            gap6_pred,\n","        ) = self(x, lenpoi)\n","\n","        loss_mask = (y[1] != 0).reshape(-1)\n","\n","\n","        loss_poi = (self.criterion(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap2 = (self.criterion(\n","            gap2_pred.reshape(-1, self.dims.g2len), y[2].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap3 = (self.criterion(\n","            gap3_pred.reshape(-1, self.dims.g3len), y[3].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap4 = (self.criterion(\n","            gap4_pred.reshape(-1, self.dims.g4len), y[4].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap5 = (self.criterion(\n","            gap5_pred.reshape(-1, self.dims.g5len), y[5].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap6 = (self.criterion(\n","            gap6_pred.reshape(-1, self.dims.g6len), y[6].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","\n","        loss = (\n","            loss_poi + loss_gap2 + loss_gap3 + loss_gap4 + loss_gap5 + loss_gap6\n","        ) / (loss_mask.sum() * 6)\n","\n","\n","        top1_acc = self.top1(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","        top5_acc = self.top5(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","        top10_acc = self.top10(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","        top20_acc = self.top20(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","        mrr = self.mrr(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","\n","        self.log(\"test/loss\", loss)\n","        self.log(\"test/loss_gap2\", loss_gap2)\n","        self.log(\"test/loss_gap3\", loss_gap3)\n","        self.log(\"test/loss_gap4\", loss_gap4)\n","        self.log(\"test/loss_gap5\", loss_gap5)\n","        self.log(\"test/loss_gap6\", loss_gap6)\n","        self.log(\"test/loss_poi\", loss_poi)\n","\n","        # log \"leaderboard\" metrics\n","        self.log(\"test/top1\", top1_acc)\n","        self.log(\"test/top5\", top5_acc)\n","        self.log(\"test/top10\", top10_acc)\n","        self.log(\"test/top20\", top20_acc)\n","        self.log(\"test/mrr\", mrr)\n","\n","        return {\"loss\": loss}\n","\n","    def configure_optimizers(self):\n","        # Define optimizer and scheduler\n","        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4, amsgrad=True)\n","        return optimizer"]},{"cell_type":"markdown","metadata":{"id":"aEkguqKVCCyv"},"source":["## Graph Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"BbiQDx04CCyv"},"outputs":[],"source":["# GNN Components\n","\n","\n","class attn_LSTM(pl.LightningModule):\n","\n","    def __init__(self, embedding_dim, hidden_dim):\n","        super(attn_LSTM, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.W = nn.Linear(embedding_dim, 4 * hidden_dim)\n","        self.U = nn.Linear(hidden_dim, 4 * hidden_dim)\n","\n","        self.s_W = nn.Linear(embedding_dim, 4 * hidden_dim)\n","        self.t_W = nn.Linear(embedding_dim, 4 * hidden_dim)\n","\n","    def forward(self, x, hidden, spatial, temporal, numTimeSteps):\n","        x_unpacked,_ = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n","\n","        h_t, c_t = hidden\n","\n","        previous_h_t = h_t\n","        previous_c_t = c_t\n","\n","        allGates_preact = (\n","            self.W(x_unpacked) + self.U(previous_h_t) + self.s_W(spatial) + self.t_W(temporal)\n","        )\n","\n","        input_g = allGates_preact[:, :, : self.hidden_dim].sigmoid()\n","        forget_g = allGates_preact[\n","            :, :, self.hidden_dim : 2 * self.hidden_dim\n","        ].sigmoid()\n","        output_g = allGates_preact[\n","            :, :, 2 * self.hidden_dim : 3 * self.hidden_dim\n","        ].sigmoid()\n","        c_t_g = allGates_preact[:, :, 3 * self.hidden_dim :].tanh()\n","\n","        c_t = forget_g * previous_c_t + input_g * c_t_g\n","        h_t = output_g * c_t.tanh()\n","\n","        batchSize = x_unpacked.shape[0]\n","        h_t = h_t.view(batchSize, numTimeSteps, self.hidden_dim)\n","        c_t = c_t.view(batchSize, numTimeSteps, self.hidden_dim)\n","\n","        return x, (h_t, c_t)\n","\n","\n","def get_neighbours(adj_matrix, poi):\n","    neigh_indices_list = []\n","    max_length = 0\n","\n","    for batch_poi in poi:\n","        batch_indices = []\n","        for single_poi in batch_poi:\n","            poi_row = adj_matrix[single_poi]\n","            neigh_indices = torch.where(poi_row == 1)[0]\n","            batch_indices.append(neigh_indices)\n","            max_length = max(max_length, len(neigh_indices))\n","\n","        neigh_indices_list.append(batch_indices)\n","\n","    padded_neigh_indices_list = []\n","    for batch_indices in neigh_indices_list:\n","        padded_batch_indices = pad_sequence(\n","            batch_indices, batch_first=True, padding_value=0\n","        )\n","        padded_neigh_indices_list.append(padded_batch_indices)\n","\n","    padded_tensor = torch.stack(padded_neigh_indices_list)\n","\n","    return padded_tensor\n","\n","\n","class GRNSelfAttention(torch.nn.Module):\n","\n","    def __init__(self, hidden_dim, n_heads):\n","\n","        super(GRNSelfAttention, self).__init__()\n","\n","        self.hidden_dim = hidden_dim\n","        self.n_heads = n_heads\n","\n","        self.Wp = nn.Linear(hidden_dim, hidden_dim)  # embeddings to pre-concat\n","        self.Wa = nn.Linear(2 * hidden_dim, hidden_dim)  # concatenation to pre-softmax\n","\n","        # total size = 3 * (hidden_dim) ** 2, quadratic in embedding size\n","\n","    def forward(self, poi, neighbors):\n","        \"\"\"forward\n","\n","        Parameters\n","        ----------\n","        poi: torch.Tensor\n","            A batched tensor of embedded POI vectors, (B x H) where H is the\n","            embedding dimension\n","        neighbors: torch.Tensor\n","            A batched tensor of sequences of embedded POI vectors that are extracted\n","            from an adjacency matrix (temporal or spatial neighbors of POI),\n","            (B x N x H), where N is the number of neighbours of POI, B is the\n","            batch size, H is the embedding dimension, and must be the same as POI\n","\n","        Returns\n","        -------\n","        tuple[torch.Tensor, torch.Tensor]\n","          A tuple containing the self-attention weighted hadamard product of neighbour activations\n","          in the first index, the attention weights in the second index.\n","        \"\"\"\n","        # assert len(poi.shape) == 2, f\"POI tensor must be 2D, got {poi.shape} instead\"\n","        assert (\n","            len(neighbors.shape) == 3\n","        ), f\"Neighbour tensor must be 3D, got {neighbors.shape} instead\"\n","\n","        B, N, H = neighbors.shape\n","\n","        h_poi = self.Wp(poi)\n","        h_n = self.Wp(neighbors)\n","        h_cat = torch.cat([h_poi.expand(B, N, -1), h_n], dim=2)\n","        h_att = F.leaky_relu(self.Wa(h_cat))\n","\n","        alpha = torch.nn.functional.softmax(h_att, dim=1)\n","\n","        p = torch.sum(alpha * h_n, dim=1)\n","        return p, alpha"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpnLqpOSCCyv"},"outputs":[],"source":["# GRN (Graph Recurrent Network)\n","class GRN(pl.LightningModule):\n","\n","    def __init__(\n","        self,\n","        dims: BaselineDimensions,\n","        spatial_graph,\n","        temporal_graph,\n","        hidden_dim,\n","        n_heads,\n","        dropout_rate=0.9,\n","        device=\"cpu\",\n","    ):\n","        super(GRN, self).__init__()\n","        self.hidden_dim = hidden_dim\n","        self.n_heads = n_heads\n","        self.dims = dims\n","\n","        self.spatial_graph = spatial_graph.to(device)\n","        self.temporal_graph = temporal_graph.to(device)\n","\n","        self.spatial_attn = GRNSelfAttention(hidden_dim, n_heads)\n","        self.temporal_attn = GRNSelfAttention(hidden_dim, n_heads)\n","\n","        self.lstm = attn_LSTM(hidden_dim, hidden_dim)\n","\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","        self.user_embedding = nn.Embedding(dims.nuser, hidden_dim, padding_idx=0)\n","        self.poi_embedding = nn.Embedding(dims.npoi, hidden_dim, padding_idx=0)\n","        self.g2_embed = nn.Embedding(dims.g2len, hidden_dim, padding_idx=0)\n","        self.g3_embed = nn.Embedding(dims.g3len, hidden_dim, padding_idx=0)\n","        self.g4_embed = nn.Embedding(dims.g4len, hidden_dim, padding_idx=0)\n","        self.g5_embed = nn.Embedding(dims.g5len, hidden_dim, padding_idx=0)\n","        self.g6_embed = nn.Embedding(dims.g6len, hidden_dim, padding_idx=0)\n","\n","        self.linear_poi = nn.Linear(2 * hidden_dim, dims.npoi)\n","        self.linear_g2 = nn.Linear(2 * hidden_dim, dims.g2len)\n","        self.linear_g3 = nn.Linear(2 * hidden_dim, dims.g3len)\n","        self.linear_g4 = nn.Linear(2 * hidden_dim, dims.g4len)\n","        self.linear_g5 = nn.Linear(2 * hidden_dim, dims.g5len)\n","        self.linear_g6 = nn.Linear(2 * hidden_dim, dims.g6len)\n","        self.top1 = AccuracyAtK(1)\n","        self.top5 = AccuracyAtK(5)\n","        self.top10 = AccuracyAtK(10)\n","        self.top20 = AccuracyAtK(20)\n","        self.mrr = MeanReciprocalRank()\n","\n","        # extract indices from one-hot neighbor list\n","        self.iota = torch.arange(self.dims.npoi, requires_grad=False, device=device)\n","\n","        self.criterion = nn.CrossEntropyLoss(reduction=\"none\")\n","\n","        self.apply(self.init_weights)\n","\n","    def init_weights(self, w):\n","        if type(w) == nn.Linear:\n","            nn.init.kaiming_normal_(w.weight)\n","            nn.init.constant_(w.bias, 0)\n","        elif type(w) == nn.LSTM:\n","            for name, param in w.named_parameters():\n","                if \"bias\" in name:\n","                    nn.init.constant_(param, 0)\n","                elif \"weight\" in name:\n","                    nn.init.kaiming_normal_(param)\n","        elif type(w) == nn.Embedding:\n","            nn.init.kaiming_normal_(w.weight)\n","            nn.init.constant_(w.weight[0], 0)\n","\n","    def forward(self, x, lengths):\n","\n","        users, poi, x_g2, x_g3, x_g4, x_g5, x_g6 = x\n","        B, T = poi.shape\n","\n","        users = users.repeat(T, 1).T\n","\n","        neighbors_spatial = self.spatial_graph[poi]\n","        neighbors_temporal = self.temporal_graph[poi]\n","\n","        e_user = self.dropout(self.user_embedding(users))\n","        e_poi = self.dropout(self.poi_embedding(poi))\n","        e_gap2 = self.dropout(self.g2_embed(x_g2))\n","        e_gap3 = self.dropout(self.g3_embed(x_g3))\n","        e_gap4 = self.dropout(self.g4_embed(x_g4))\n","        e_gap5 = self.dropout(self.g5_embed(x_g5))\n","        e_gap6 = self.dropout(self.g6_embed(x_g6))\n","\n","        spatial_atts = torch.empty((B, T, self.hidden_dim), device=self.device)\n","        temporal_atts = torch.empty((B, T, self.hidden_dim), device=self.device)\n","\n","        for b in range(B):\n","            for t in range(T):\n","\n","                spatial_neigh = neighbors_spatial[b, t] * self.iota\n","                temporal_neigh = neighbors_temporal[b, t] * self.iota\n","\n","                spatial_neigh = spatial_neigh[spatial_neigh != 0]\n","                temporal_neigh = temporal_neigh[temporal_neigh != 0]\n","\n","                spatial_neigh = spatial_neigh.unsqueeze(0)\n","                temporal_neigh = temporal_neigh.unsqueeze(0)\n","\n","                e_spatial = self.dropout(self.poi_embedding(spatial_neigh))\n","                e_temporal = self.dropout(self.poi_embedding(temporal_neigh))\n","\n","                curr_poi = e_poi[b, t].unsqueeze(0)\n","\n","                spatial_p, _ = self.spatial_attn(curr_poi, e_spatial)\n","                temporal_p, _ = self.temporal_attn(curr_poi, e_temporal)\n","\n","                # we are not using the batch dimension, so we squeeze it\n","                spatial_atts[b, t] = spatial_p.squeeze()\n","                temporal_atts[b, t] = temporal_p.squeeze()\n","\n","        # zero-init LSTM states\n","        h_t = torch.zeros(B, T, self.hidden_dim, device=self.device)\n","        c_t = torch.zeros(B, T, self.hidden_dim, device=self.device)\n","\n","        packed_input = nn.utils.rnn.pack_padded_sequence(\n","            e_poi, lengths, batch_first=True, enforce_sorted=False\n","        )\n","        pack_out, (h_t, c_t) = self.lstm(\n","            packed_input, (h_t, c_t), spatial_atts, temporal_atts, T\n","        )\n","        output, _ = nn.utils.rnn.pad_packed_sequence(\n","            pack_out, batch_first=True\n","        )\n","\n","        # Note:the prediction of the poi depends on the embedding of the user\n","        next_poi = self.linear_poi(torch.cat((output, e_user), dim=2))\n","        next_g2 = self.linear_g2(torch.cat((output, e_gap2), dim=2))\n","        next_g3 = self.linear_g3(torch.cat((output, e_gap3), dim=2))\n","        next_g4 = self.linear_g4(torch.cat((output, e_gap4), dim=2))\n","        next_g5 = self.linear_g5(torch.cat((output, e_gap5), dim=2))\n","        next_g6 = self.linear_g6(torch.cat((output, e_gap6), dim=2))\n","\n","        return next_poi, next_g2, next_g3, next_g4, next_g5, next_g6\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y, len_x = batch\n","        (\n","            poi_pred,\n","            gap2_pred,\n","            gap3_pred,\n","            gap4_pred,\n","            gap5_pred,\n","            gap6_pred,\n","        ) = self(x, len_x)\n","\n","        loss_mask = (y[1] != 0).reshape(-1)\n","        loss_poi = (\n","            self.criterion(poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap2 = (\n","            self.criterion(gap2_pred.reshape(-1, self.dims.g2len), y[2].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap3 = (\n","            self.criterion(gap3_pred.reshape(-1, self.dims.g3len), y[3].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap4 = (\n","            self.criterion(gap4_pred.reshape(-1, self.dims.g4len), y[4].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap5 = (\n","            self.criterion(gap5_pred.reshape(-1, self.dims.g5len), y[5].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap6 = (\n","            self.criterion(gap6_pred.reshape(-1, self.dims.g6len), y[6].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","\n","        loss = (\n","            (loss_poi + loss_gap2 + loss_gap3 + loss_gap4 + loss_gap5 + loss_gap6)\n","            / loss_mask.sum()\n","            * 6\n","        )\n","\n","        self.log(\"train/loss\", loss)\n","        self.log(\"train/loss_gap2\", loss_gap2)\n","        self.log(\"train/loss_gap3\", loss_gap3)\n","        self.log(\"train/loss_gap4\", loss_gap4)\n","        self.log(\"train/loss_gap5\", loss_gap5)\n","        self.log(\"train/loss_gap6\", loss_gap6)\n","        self.log(\"train/loss_poi\", loss_poi)\n","\n","        return {\"loss\": loss}\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y, len_x = batch\n","\n","\n","\n","        (\n","            poi_pred,\n","            gap2_pred,\n","            gap3_pred,\n","            gap4_pred,\n","            gap5_pred,\n","            gap6_pred,\n","        ) = self(x, len_x)\n","\n","\n","        loss_mask = (y[1] != 0).reshape(-1)\n","        loss_poi = (\n","            self.criterion(poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap2 = (\n","            self.criterion(gap2_pred.reshape(-1, self.dims.g2len), y[2].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap3 = (\n","            self.criterion(gap3_pred.reshape(-1, self.dims.g3len), y[3].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap4 = (\n","            self.criterion(gap4_pred.reshape(-1, self.dims.g4len), y[4].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap5 = (\n","            self.criterion(gap5_pred.reshape(-1, self.dims.g5len), y[5].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap6 = (\n","            self.criterion(gap6_pred.reshape(-1, self.dims.g6len), y[6].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","\n","        loss = (\n","            (loss_poi + loss_gap2 + loss_gap3 + loss_gap4 + loss_gap5 + loss_gap6)\n","            / loss_mask.sum()\n","            * 6\n","        )\n","\n","        top1_acc = self.top1(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","        top5_acc = self.top5(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","        top10_acc = self.top10(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","        top20_acc = self.top20(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","        mrr = self.mrr(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","\n","        self.log(\"val/loss\", loss)\n","        self.log(\"val/loss_gap2\", loss_gap2)\n","        self.log(\"val/loss_gap3\", loss_gap3)\n","        self.log(\"val/loss_gap4\", loss_gap4)\n","        self.log(\"val/loss_gap5\", loss_gap5)\n","        self.log(\"val/loss_gap6\", loss_gap6)\n","        self.log(\"val/loss_poi\", loss_poi)\n","\n","        # log \"leaderboard\" metrics\n","        self.log(\"val/top1\", top1_acc)\n","        self.log(\"val/top5\", top5_acc)\n","        self.log(\"val/top10\", top10_acc)\n","        self.log(\"val/top20\", top20_acc)\n","        self.log(\"val/mrr\", mrr)\n","\n","        return loss\n","\n","    def test_step(self, batch, batch_idx):\n","        x, y, len_x = batch\n","\n","\n","\n","        (\n","            poi_pred,\n","            gap2_pred,\n","            gap3_pred,\n","            gap4_pred,\n","            gap5_pred,\n","            gap6_pred,\n","        ) = self(x, len_x)\n","\n","\n","\n","        loss_mask = (y[1] != 0).reshape(-1)\n","\n","        loss_poi = (\n","            self.criterion(poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap2 = (\n","            self.criterion(gap2_pred.reshape(-1, self.dims.g2len), y[2].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap3 = (\n","            self.criterion(gap3_pred.reshape(-1, self.dims.g3len), y[3].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap4 = (\n","            self.criterion(gap4_pred.reshape(-1, self.dims.g4len), y[4].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap5 = (\n","            self.criterion(gap5_pred.reshape(-1, self.dims.g5len), y[5].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","        loss_gap6 = (\n","            self.criterion(gap6_pred.reshape(-1, self.dims.g6len), y[6].reshape(-1))\n","            .where(loss_mask, torch.tensor(0.0))\n","            .sum()\n","        )\n","\n","        loss = (\n","            (loss_poi + loss_gap2 + loss_gap3 + loss_gap4 + loss_gap5 + loss_gap6)\n","            / loss_mask.sum()\n","            * 6\n","        )\n","\n","        top1_acc = self.top1(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","        top5_acc = self.top5(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","        top10_acc = self.top10(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","        top20_acc = self.top20(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","        mrr = self.mrr(\n","            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1), loss_mask\n","        )\n","        self.log(\"test/loss\", loss)\n","        self.log(\"test/loss_gap2\", loss_gap2)\n","        self.log(\"test/loss_gap3\", loss_gap3)\n","        self.log(\"test/loss_gap4\", loss_gap4)\n","        self.log(\"test/loss_gap5\", loss_gap5)\n","        self.log(\"test/loss_gap6\", loss_gap6)\n","        self.log(\"test/loss_poi\", loss_poi)\n","\n","        # log \"leaderboard\" metrics\n","        self.log(\"test/top1\", top1_acc)\n","        self.log(\"test/top5\", top5_acc)\n","        self.log(\"test/top10\", top10_acc)\n","        self.log(\"test/top20\", top20_acc)\n","        self.log(\"test/mrr\", mrr)\n","\n","        return {\"loss\": loss}\n","\n","    def configure_optimizers(self):\n","        # Define optimizer and scheduler\n","        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4, amsgrad=True)\n","        return optimizer"]},{"cell_type":"markdown","metadata":{"id":"kRG9Fg0sCCyw"},"source":["## Training Loops"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4B3gEpRhCCyw"},"outputs":[],"source":["n_users = encoder_dict[\"users\"].classes_.shape[0]\n","n_pois = encoder_dict[\"pois\"].classes_.shape[0]\n","n_g2 = encoder_dict[\"g2\"].classes_.shape[0]\n","n_g3 = encoder_dict[\"g3\"].classes_.shape[0]\n","n_g4 = encoder_dict[\"g4\"].classes_.shape[0]\n","n_g5 = encoder_dict[\"g5\"].classes_.shape[0]\n","n_g6 = encoder_dict[\"g6\"].classes_.shape[0]\n","\n","\n","# account for the padding token\n","dims = BaselineDimensions(\n","    n_users + 1, n_pois + 1, n_g2 + 1, n_g3 + 1, n_g4 + 1, n_g5 + 1, n_g6 + 1\n",")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJsxdOY5CCyw"},"outputs":[],"source":["from lightning.pytorch.loggers import WandbLogger\n","from lightning.pytorch import Trainer\n","\n","TRAIN_BASELINE = True\n","\n","wandb.finish()\n","torch.cuda.empty_cache()\n","# cargo-cult like stuff that is supposed to make you faster\n","torch.set_float32_matmul_precision(\"medium\")\n","torch.backends.cudnn.benchmark = True\n","\n","ds = CheckinModule(encoded_data_train, encoded_data_test, batch_size=32, workers=4)\n","\n","\n","wandb.init(project=\"trovailpoi\")\n","\n","classifier_baseline = HMT_RN(dims, embedding_dim=1024, lstm_hidden_dim=1024)\n","wandb_logger = WandbLogger(project=\"trovailpoi\")\n","trainer = Trainer(\n","    max_epochs=200,\n","    accelerator=\"auto\",\n","    devices=[0],\n","    log_every_n_steps=10,\n","    logger=wandb_logger,\n","    strategy=\"auto\",\n","    callbacks=[\n","        torchpl.callbacks.LearningRateMonitor(logging_interval=\"step\"),\n","        torchpl.callbacks.ModelCheckpoint(\n","            monitor=\"val/loss\",\n","            mode=\"min\",\n","            save_top_k=1,\n","            save_last=True,\n","            filename=\"best_model\",\n","        ),\n","        torchpl.callbacks.EarlyStopping(\n","            monitor=\"val/loss\", patience=3, min_delta=0.0005, mode=\"min\"\n","        ),\n","    ],\n",")\n","\n","if TRAIN_BASELINE:\n","    trainer.fit(model=classifier_baseline, datamodule=ds)\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZltK8HPhCCyw"},"outputs":[],"source":["from lightning.pytorch.loggers import WandbLogger\n","from lightning.pytorch import Trainer\n","\n","TRAIN_GNN = True\n","\n","\n","wandb.finish()\n","torch.cuda.empty_cache()\n","# cargo-cult like stuff that is supposed to make you faster\n","torch.set_float32_matmul_precision(\"medium\")\n","torch.backends.cudnn.benchmark = True\n","\n","ds = CheckinModule(encoded_data_train, encoded_data_test, batch_size=32, workers=4)\n","\n","\n","wandb.init(project=\"trovailpoi\")\n","\n","classifier_gnn = GRN(\n","    dims,\n","    spatial_graph,\n","    temporal_graph,\n","    hidden_dim=1024,\n","    n_heads=1,\n","    dropout_rate=0.9,\n","    device=device,\n",")\n","wandb_logger = WandbLogger(project=\"trovailpoi\")\n","trainer = Trainer(\n","    max_epochs=200,\n","    accelerator=\"auto\",\n","    devices=[0],\n","    log_every_n_steps=10,\n","    logger=wandb_logger,\n","    strategy=\"auto\",\n","    callbacks=[\n","        torchpl.callbacks.LearningRateMonitor(logging_interval=\"step\"),\n","        torchpl.callbacks.ModelCheckpoint(\n","            monitor=\"val/loss\",\n","            mode=\"min\",\n","            save_top_k=1,\n","            save_last=True,\n","            filename=\"best_model\",\n","        ),\n","        torchpl.callbacks.EarlyStopping(\n","            monitor=\"val/loss\", patience=3, min_delta=0.0005, mode=\"min\"\n","        ),\n","    ],\n",")\n","\n","if TRAIN_GNN:\n","    trainer.fit(model=classifier_gnn, datamodule=ds)\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"abzS8kwAKPPu"},"outputs":[],"source":["checkpoint_path = \"/content/trovailpoi/dnpotgxc/checkpoints/best_model.ckpt\"\n","# Load the trained model from the checkpoint\n","trained_model = HMT_RN.load_from_checkpoint(\n","    checkpoint_path,\n","    dimensions=dims,\n","    embedding_dim=1024,  # Example embedding dimension\n","    lstm_hidden_dim=1024,  # Example LSTM hidden dimension\n","    dropout_rate=0.9,  # Example dropout rate\n",")\n","\n","# Create a test dataloader\n","# Assuming you have a method `test_dataloader` in your data module\n","test_loader = ds.test_dataloader()  # Replace `ds` with your actual data module instance\n","\n","# Instantiate the trainer\n","trainer = Trainer(accelerator=\"auto\", devices=[0])\n","\n","# Test the model\n","results = trainer.test(trained_model, test_loader)\n","\n","# Print the test results\n","print(results)"]},{"cell_type":"markdown","metadata":{"id":"ik7z0iyRCCyw"},"source":["## Scrapbook for Experimentation\n","\n","Ignore all code below, it's just for quick prototyping"]},{"cell_type":"markdown","metadata":{"id":"WFzGUP5-ZPM2"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4985565,"sourceId":8383172,"sourceType":"datasetVersion"},{"datasetId":5026368,"sourceId":8438278,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
