{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as rs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as pl\n",
    "import lightning.pytorch as torchpl\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"user\", \"poi\", \"date\", \"TZ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = rs.read_csv(\n",
    "    \"dataset_TIST2015/dataset_TIST2015_Checkins.txt\",\n",
    "    has_header=False,\n",
    "    low_memory=True,\n",
    "    separator=\"\\t\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_users = (\n",
    "    data.lazy()\n",
    "    .group_by(\"user\")\n",
    "    .agg(\n",
    "        [\n",
    "            rs.col(\"poi\").n_unique().alias(\"n_pois\"),\n",
    "            rs.col(\"poi\").count().alias(\"n_checkins\"),\n",
    "            # turn the rest into a list\n",
    "            rs.col(\"poi\").alias(\"pois\"),\n",
    "            rs.col(\"date\").alias(\"dates\"),\n",
    "            rs.col(\"TZ\").alias(\"TZs\"),\n",
    "        ]\n",
    "    )\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_users.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_culled = data_users.filter(\n",
    "    (rs.col(\"n_checkins\") > 20) & (rs.col(\"n_checkins\") < 50)\n",
    ").drop_nulls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del data_users\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract unique elements from each lists in data_culled[\"pois\"]\n",
    "out = data_culled.with_columns(\n",
    "    [\n",
    "        rs.col(\"pois\").list.unique(),\n",
    "        rs.col(\"pois\").list.unique().list.len().alias(\"n_unique_pois\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_culled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = out[\"pois\"][0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = data_culled[\"pois\"][0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pois = out[\"pois\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_pois = unique_pois.list.explode().value_counts().filter(rs.col(\"count\") >= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_pois = frequent_pois[\"pois\"]\n",
    "frequent_pois = set(frequent_pois.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_culled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_culled = data_culled.with_columns(\n",
    "    [\n",
    "        rs.col(\"pois\")\n",
    "        .list.eval(\n",
    "            rs.element().is_in(frequent_pois),\n",
    "        )\n",
    "        .alias(\"is_frequent\")\n",
    "    ]\n",
    ")  # prep mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = (\n",
    "    data_culled.lazy()\n",
    "    .with_row_index()\n",
    "    .explode(\n",
    "        [\n",
    "            \"pois\",\n",
    "            \"dates\",\n",
    "            \"TZs\",\n",
    "            \"is_frequent\",\n",
    "        ]\n",
    "    )\n",
    "    .group_by(\"user\")\n",
    "    .agg(\n",
    "        [\n",
    "            \n",
    "            rs.col(\"pois\").filter(rs.col(\"is_frequent\")).alias(\"pois\"),\n",
    "            rs.col(\"dates\").filter(rs.col(\"is_frequent\")).alias(\"dates\"),\n",
    "            rs.col(\"TZs\").filter(rs.col(\"is_frequent\")).alias(\"TZs\"),\n",
    "            rs.col(\"pois\").filter(rs.col(\"is_frequent\")).n_unique().alias(\"n_pois\"),\n",
    "            rs.col(\"pois\").filter(rs.col(\"is_frequent\")).count().alias(\"n_checkins\"),\n",
    "        ]\n",
    "    )\n",
    "    .filter(rs.col(\"n_checkins\") > 0)\n",
    "    .filter(rs.col(\"n_pois\") > 0)\n",
    "    .collect()\n",
    ")  # filter out infrequent pois and users with no pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geohash2 as gh\n",
    "\n",
    "pois = rs.read_csv(\n",
    "    \"dataset_TIST2015/dataset_TIST2015_POIs.txt\",\n",
    "    has_header=False,\n",
    "    low_memory=True,\n",
    "    separator=\"\\t\",\n",
    ")\n",
    "pois.columns = [\"poi\", \"lat\", \"long\", \"category\", \"country\"]\n",
    "pois = pois.drop(\"category\").drop(\"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois = (\n",
    "    pois.lazy()\n",
    "    .filter(rs.col(\"poi\").is_in(frequent_pois))\n",
    "    .select(\n",
    "        [\n",
    "            rs.col(\"poi\"),\n",
    "            rs.struct(\n",
    "                [\n",
    "                    rs.col(\"lat\").cast(rs.Float32),\n",
    "                    rs.col(\"long\").cast(rs.Float32),\n",
    "                ]\n",
    "            )\n",
    "            .alias(\"location\")\n",
    "            .map_elements(\n",
    "                lambda s: gh.encode(s[\"lat\"], s[\"long\"], precision=6),\n",
    "                return_dtype=rs.String,\n",
    "            )\n",
    "            .alias(\"geohash\"),\n",
    "        ]\n",
    "    )\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_geo_dict = dict(zip(pois[\"poi\"], pois[\"geohash\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each row in final_data, add the geohash of the pois\n",
    "\n",
    "\n",
    "final_data = final_data.with_columns(\n",
    "    [\n",
    "        rs.col(\"pois\")\n",
    "        .map_elements(\n",
    "            lambda s: [poi_geo_dict[s] for s in s],\n",
    "        )\n",
    "        .alias(\"geohashes\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[\"dates\"][79].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[\"TZs\"][79].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "def UTC_to_local(utc, tz):\n",
    "\n",
    "    date = datetime.datetime.strptime(utc, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    date = date.replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "    # shift by tz offset\n",
    "\n",
    "    date = date.astimezone(datetime.timezone(datetime.timedelta(minutes=tz)))\n",
    "\n",
    "    date_s = datetime.datetime.strftime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return date_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UTC_to_local(\"Mon May 21 15:53:01 +0000 2012\", -420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.with_columns(\n",
    "    [\n",
    "        rs.struct([rs.col(\"dates\"), rs.col(\"TZs\")])\n",
    "        .alias(\"times\")\n",
    "        .map_elements(\n",
    "            lambda struct: [\n",
    "                UTC_to_local(date, tz)\n",
    "                for date, tz in zip(struct[\"dates\"], struct[\"TZs\"])\n",
    "            ],\n",
    "            return_dtype=rs.List(rs.String),\n",
    "        )\n",
    "    ]\n",
    ")   # This *should* perform timezone conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_UNIX_time(date):\n",
    "    return datetime.datetime.strptime(date, \"%Y-%m-%d %H:%M:%S\").timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sorted = final_data.select( # sort the times\n",
    "    [\n",
    "        rs.col(\"user\"),\n",
    "        rs.struct(\n",
    "            [\n",
    "                rs.col(\"pois\"),\n",
    "                rs.col(\"times\"),\n",
    "            ]\n",
    "        ).map_elements(\n",
    "            lambda struct: [\n",
    "                poi\n",
    "                for poi, _ in sorted(\n",
    "                    zip(\n",
    "                        struct[\"pois\"], [to_UNIX_time(date) for date in struct[\"times\"]]\n",
    "                    ),\n",
    "                    key=lambda s: s[1],\n",
    "                )\n",
    "            ],\n",
    "            return_dtype=rs.List(rs.String),\n",
    "        ),\n",
    "        rs.struct(\n",
    "            [\n",
    "                rs.col(\"geohashes\"),\n",
    "                rs.col(\"times\"),\n",
    "            ]\n",
    "        ).map_elements(\n",
    "            lambda struct: [\n",
    "                geo\n",
    "                for geo, _ in sorted(\n",
    "                    zip(\n",
    "                        struct[\"geohashes\"],\n",
    "                        [to_UNIX_time(date) for date in struct[\"times\"]],\n",
    "                    ),\n",
    "                    key=lambda s: s[1],\n",
    "                )\n",
    "            ],\n",
    "            return_dtype=rs.List(rs.String),\n",
    "        ),\n",
    "        rs.col(\"times\")\n",
    "        .map_elements(lambda dates: sorted(dates, key=to_UNIX_time), return_dtype=rs.List(rs.String))\n",
    "        .alias(\"times_sorted\"),\n",
    "        rs.col(\"n_checkins\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we now need to obtain a dataframe containing: each POI, it's geohash, and a set of all the check-ins it appears in\n",
    "\n",
    "pois_checkins = final_sorted.explode([\"pois\", \"geohashes\"]).drop(\"n_checkins\")\n",
    "\n",
    "pois_checkins = pois_checkins.with_columns(\n",
    "    [\n",
    "        rs.col(\"geohashes\").map_elements(lambda s: s[:4], rs.String).alias(\"g4\"),\n",
    "    ]\n",
    ").drop(\"geohashes\").group_by([\"pois\", \"g4\"]).agg(\n",
    "    [\n",
    "        rs.col(\"times_sorted\").flatten().alias(\"checkin_times\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_checkins # with this we can *efficiently* build our POI-POI spatial-temporal graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UTC_to_weekslot(utc: str) -> int:\n",
    "    # convert UTC into an integer (from 0 to 55), according to which three-hour slot\n",
    "    # it occupies in a week\n",
    "    \n",
    "    date = datetime.datetime.strptime(utc, \"%Y-%m-%d %H:%M:%S\")\n",
    "    week = date.weekday()\n",
    "    hour = date.hour\n",
    "    \n",
    "    return week * 4 + hour // 3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dict = {\n",
    "    \"users\": LabelEncoder(),\n",
    "    \"pois\": LabelEncoder(),\n",
    "    \"g2\": LabelEncoder(),\n",
    "    \"g3\": LabelEncoder(),\n",
    "    \"g4\": LabelEncoder(),\n",
    "    \"g5\": LabelEncoder(),\n",
    "    \"g6\": LabelEncoder(),\n",
    "}\n",
    "\n",
    "encoded_data = {\n",
    "    \"users\" : [],\n",
    "    \"pois\" : [],\n",
    "    \"g2\" : [],\n",
    "    \"g3\" : [],\n",
    "    \"g4\" : [],\n",
    "    \"g5\" : [],\n",
    "    \"g6\" : [],\n",
    "}\n",
    "\n",
    "unique_data = {\n",
    "    \"users\" : set(),\n",
    "    \"pois\" : set(),\n",
    "    \"g2\" : set(),\n",
    "    \"g3\" : set(),\n",
    "    \"g4\" : set(),\n",
    "    \"g5\" : set(),\n",
    "    \"g6\" : set(),\n",
    "}\n",
    "\n",
    "# quick and dirty encoding:\n",
    "# 1. put every unique symbol in a list\n",
    "# 2. fit the respective encoder\n",
    "# 3. transform the lists\n",
    "\n",
    "for i, row in enumerate(final_sorted.iter_rows()):\n",
    "\n",
    "    user, pois, geohashes, times_sorted, n_checkins = row\n",
    "    \n",
    "    g2 = [geo[:2] for geo in geohashes]\n",
    "    g3 = [geo[:3] for geo in geohashes]\n",
    "    g4 = [geo[:4] for geo in geohashes]\n",
    "    g5 = [geo[:5] for geo in geohashes]\n",
    "    g6 = [geo[:6] for geo in geohashes] # redundant, but I like symmetry\n",
    "    \n",
    "    unique_data[\"users\"].add(user)\n",
    "    unique_data[\"pois\"].update(pois)\n",
    "    unique_data[\"g2\"].update(g2)\n",
    "    unique_data[\"g3\"].update(g3)\n",
    "    unique_data[\"g4\"].update(g4)\n",
    "    unique_data[\"g5\"].update(g5)\n",
    "    unique_data[\"g6\"].update(g6)\n",
    "\n",
    "for property, enc, data in zip(encoder_dict.keys(), encoder_dict.values(), unique_data.values()):\n",
    "    enc.fit(list(data))\n",
    "    encoder_dict[property] = enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could be optimized, right now it takes a while\n",
    "\n",
    "for i, row in tqdm(enumerate(final_sorted.iter_rows())):\n",
    "    \n",
    "    user, pois, geohashes, times_sorted, n_checkins = row\n",
    "    \n",
    "    g2 = [geo[:2] for geo in geohashes]\n",
    "    g3 = [geo[:3] for geo in geohashes]\n",
    "    g4 = [geo[:4] for geo in geohashes]\n",
    "    g5 = [geo[:5] for geo in geohashes]\n",
    "    g6 = [geo[:6] for geo in geohashes]\n",
    "    \n",
    "    encoded_data[\"users\"].append(encoder_dict[\"users\"].transform([user])[0])\n",
    "    encoded_data[\"pois\"].append(encoder_dict[\"pois\"].transform(pois))\n",
    "    encoded_data[\"g2\"].append(encoder_dict[\"g2\"].transform(g2))\n",
    "    encoded_data[\"g3\"].append(encoder_dict[\"g3\"].transform(g3))\n",
    "    encoded_data[\"g4\"].append(encoder_dict[\"g4\"].transform(g4))\n",
    "    encoded_data[\"g5\"].append(encoder_dict[\"g5\"].transform(g5))\n",
    "    encoded_data[\"g6\"].append(encoder_dict[\"g6\"].transform(g6))\n",
    "    \n",
    "    # sum 1 to all values to avoid 0s\n",
    "    encoded_data[\"users\"][-1] += 1\n",
    "    encoded_data[\"pois\"][-1] += 1\n",
    "    encoded_data[\"g2\"][-1] += 1\n",
    "    encoded_data[\"g3\"][-1] += 1\n",
    "    encoded_data[\"g4\"][-1] += 1\n",
    "    encoded_data[\"g5\"][-1] += 1\n",
    "    encoded_data[\"g6\"][-1] += 1\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_checkins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also encode the graph dataframe so we can build the graphs\n",
    "\n",
    "pois_checkins = pois_checkins.lazy().with_columns(\n",
    "    [\n",
    "        rs.col(\"pois\").map_elements(lambda s: encoder_dict[\"pois\"].transform([s])[0], rs.Int64),\n",
    "        rs.col(\"g4\").map_elements(lambda s: encoder_dict[\"g4\"].transform([s])[0], rs.Int64), # apply utc_to_weekslot to each timestamp in the list\n",
    "        rs.col(\"checkin_times\").map_elements(lambda s: [UTC_to_weekslot(date) for date in s], rs.List(rs.Int64)),\n",
    "    ]\n",
    ").sort(\"pois\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_checkins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_row = np.array(pois_checkins[\"g4\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_graph = np.zeros((spatial_row.shape[0], spatial_row.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, a in enumerate(spatial_row):\n",
    "    for j, b in enumerate(spatial_row):\n",
    "        if a == b:\n",
    "            spatial_graph[i, j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_row = pois_checkins[\"checkin_times\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_graph = np.zeros((spatial_row.shape[0], spatial_row.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iaccard_sim(A, B):\n",
    "    I = A.intersection(B)\n",
    "    U = A.union(B)\n",
    "    \n",
    "    IoU = len(I)/len(U)\n",
    "    \n",
    "    return IoU >= 0.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, a in enumerate(temporal_row):\n",
    "    for j, b in enumerate(temporal_row):\n",
    "        if iaccard_sim(set(a), set(b)):\n",
    "            temporal_graph[i, j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def rnn_collation_fn(batch):\n",
    "    users = []\n",
    "    pois = []\n",
    "    g2 = []\n",
    "    g3 = []\n",
    "    g4 = []\n",
    "    g5 = []\n",
    "    g6 = []\n",
    "    \n",
    "    \n",
    "    for user, poi, geo2, geo3, geo4, geo5, geo6 in batch:\n",
    "        users.append(user) # 0 is reserved for padding\n",
    "        pois.append(poi)\n",
    "        g2.append(geo2)\n",
    "        g3.append(geo3)\n",
    "        g4.append(geo4)\n",
    "        g5.append(geo5)\n",
    "        g6.append(geo6)\n",
    "    seq = (\n",
    "        users,\n",
    "        pad_sequence(pois, batch_first=True, padding_value=0),\n",
    "        pad_sequence(g2, batch_first=True, padding_value=0),\n",
    "        pad_sequence(g3, batch_first=True, padding_value=0),\n",
    "        pad_sequence(g4, batch_first=True, padding_value=0),\n",
    "        pad_sequence(g5, batch_first=True, padding_value=0),\n",
    "        pad_sequence(g6, batch_first=True, padding_value=0),\n",
    "    )\n",
    "    \n",
    "    x = (\n",
    "        seq[0],\n",
    "        seq[1][:, :-1],\n",
    "        seq[2][:, :-1],\n",
    "        seq[3][:, :-1],\n",
    "        seq[4][:, :-1],\n",
    "        seq[5][:, :-1],\n",
    "        seq[6][:, :-1],\n",
    "    )\n",
    "    \n",
    "    y = (\n",
    "        seq[0],\n",
    "        seq[1][:, 1:],\n",
    "        seq[2][:, 1:],\n",
    "        seq[3][:, 1:],\n",
    "        seq[4][:, 1:],\n",
    "        seq[5][:, 1:],\n",
    "        seq[6][:, 1:],\n",
    "    \n",
    "    )\n",
    "\n",
    "    return x, y\n",
    "\n",
    "class CheckinDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"users\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        x = (\n",
    "            torch.tensor(encoded_data[\"users\"][idx], dtype=torch.long),\n",
    "            torch.tensor(encoded_data[\"pois\"][idx], dtype=torch.long),\n",
    "            torch.tensor(encoded_data[\"g2\"][idx], dtype=torch.long),\n",
    "            torch.tensor(encoded_data[\"g3\"][idx], dtype=torch.long),\n",
    "            torch.tensor(encoded_data[\"g4\"][idx], dtype=torch.long),\n",
    "            torch.tensor(encoded_data[\"g5\"][idx], dtype=torch.long),\n",
    "            torch.tensor(encoded_data[\"g6\"][idx], dtype=torch.long),\n",
    "        )\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = CheckinDataset(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(\n",
    "    ds, batch_size=8, shuffle=True, collate_fn=rnn_collation_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1][7]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
