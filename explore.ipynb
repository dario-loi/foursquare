{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foursquare dataset next-POI Recommendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off we import all the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as rs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as pl\n",
    "import lightning.pytorch as torchpl\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dataclasses import dataclass\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# define WANDB_NOTEBOOK_NAME\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"dario_nb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "# clean CUDA memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# sometimes jupyter notebook does not release memory, we leave this here so a run-all\n",
    "# can *sometimes* fix leaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load the data, we utilize `polars` since it is much more efficient than `pandas` and can handle large datasets with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"user\", \"poi\", \"date\", \"TZ\"]\n",
    "data = rs.read_csv(\n",
    "    \"dataset_TIST2015/dataset_TIST2015_Checkins.txt\",\n",
    "    has_header=False,\n",
    "    low_memory=True,\n",
    "    separator=\"\\t\",\n",
    ")\n",
    "data.columns = columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differently from what suggested by the professor, we utilize the full TIST2015 dataset, which has a far greater scale compared to the reduced NY one. However, by following the pruning steps detailed in the paper (http://dx.doi.org/10.1145/3477495.3531989, section 5.1), we obtain sequences that are much smaller in size, resulting in a dataset that is usable on Google Colab's free tier (as required by the assignment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_users = (\n",
    "    data.lazy()\n",
    "    .group_by(\"user\")\n",
    "    .agg(\n",
    "        [\n",
    "            rs.col(\"poi\").n_unique().alias(\"n_pois\"),\n",
    "            rs.col(\"poi\").count().alias(\"n_checkins\"),\n",
    "            # turn the rest into a list\n",
    "            rs.col(\"poi\").alias(\"pois\"),\n",
    "            rs.col(\"date\").alias(\"dates\"),\n",
    "            rs.col(\"TZ\").alias(\"TZs\"),\n",
    "        ]\n",
    "    )\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_users.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_culled = data_users.filter(\n",
    "    (rs.col(\"n_checkins\") > 20) & (rs.col(\"n_checkins\") < 50)\n",
    ").drop_nulls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the original dataset is huge, we delete it and call the python garbage collector to free up memory. We then proceed with the second pruning step (frequency-based pruning) as detailed in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del data_users\n",
    "\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract unique elements from each lists in data_culled[\"pois\"]\n",
    "out = data_culled.with_columns(\n",
    "    [\n",
    "        rs.col(\"pois\").list.unique(),\n",
    "        rs.col(\"pois\").list.unique().list.len().alias(\"n_unique_pois\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = out[\"pois\"][0].to_list()\n",
    "len(set(l))  # print number of unique POIs in first sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = data_culled[\"pois\"][0].to_list()\n",
    "len(l2)  # print sequence length of first user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(l2))  # confirm that the two match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a Polars query to obtain all the frequent POIs, the ones expected to survive the filtering\n",
    "unique_pois = out[\"pois\"]\n",
    "frequent_pois = unique_pois.list.explode().value_counts().filter(rs.col(\"count\") >= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_pois = frequent_pois[\"pois\"]\n",
    "frequent_pois = set(frequent_pois.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_culled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_culled = data_culled.with_columns(\n",
    "    [\n",
    "        rs.col(\"pois\")\n",
    "        .list.eval(\n",
    "            rs.element().is_in(frequent_pois),\n",
    "        )\n",
    "        .alias(\"is_frequent\")\n",
    "    ]\n",
    ")  # prep mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = (\n",
    "    data_culled.lazy()\n",
    "    .with_row_index()\n",
    "    .explode(\n",
    "        [\n",
    "            \"pois\",\n",
    "            \"dates\",\n",
    "            \"TZs\",\n",
    "            \"is_frequent\",\n",
    "        ]\n",
    "    )\n",
    "    .group_by(\"user\")\n",
    "    .agg(\n",
    "        [\n",
    "            rs.col(\"pois\").filter(rs.col(\"is_frequent\")).alias(\"pois\"),\n",
    "            rs.col(\"dates\").filter(rs.col(\"is_frequent\")).alias(\"dates\"),\n",
    "            rs.col(\"TZs\").filter(rs.col(\"is_frequent\")).alias(\"TZs\"),\n",
    "            rs.col(\"pois\").filter(rs.col(\"is_frequent\")).n_unique().alias(\"n_pois\"),\n",
    "            rs.col(\"pois\").filter(rs.col(\"is_frequent\")).count().alias(\"n_checkins\"),\n",
    "        ]\n",
    "    )\n",
    "    .filter(rs.col(\"n_checkins\") > 0)\n",
    "    .filter(rs.col(\"n_pois\") > 0)\n",
    "    .collect()\n",
    ")  # filter out infrequent pois and users with no pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, culling is done, we can appreciate that `polars`'s SQL/functional-style API is different from Pandas, but it is very powerful and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is geohashing the POIs, that is, we want to convert the latitude-longitude positions of the POIs into a grid-based geohash representation, which will form the basis for our network's embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geohash2 as gh\n",
    "\n",
    "pois = rs.read_csv(\n",
    "    \"dataset_TIST2015/dataset_TIST2015_POIs.txt\",\n",
    "    has_header=False,\n",
    "    low_memory=True,\n",
    "    separator=\"\\t\",\n",
    ")\n",
    "pois.columns = [\"poi\", \"lat\", \"long\", \"category\", \"country\"]\n",
    "pois = pois.drop(\"category\").drop(\"country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois = (\n",
    "    pois.lazy()\n",
    "    .filter(rs.col(\"poi\").is_in(frequent_pois))\n",
    "    .select(\n",
    "        [\n",
    "            rs.col(\"poi\"),\n",
    "            rs.struct(\n",
    "                [\n",
    "                    rs.col(\"lat\").cast(rs.Float32),\n",
    "                    rs.col(\"long\").cast(rs.Float32),\n",
    "                ]\n",
    "            )\n",
    "            .alias(\"location\")\n",
    "            .map_elements(\n",
    "                lambda s: gh.encode(s[\"lat\"], s[\"long\"], precision=6),\n",
    "                return_dtype=rs.String,\n",
    "            )\n",
    "            .alias(\"geohash\"),\n",
    "        ]\n",
    "    )\n",
    "    .collect()\n",
    ")\n",
    "poi_geo_dict = dict(zip(pois[\"poi\"], pois[\"geohash\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each row in final_data, add the geohash of the pois by hitting the poi_geo_dict\n",
    "\n",
    "final_data = final_data.with_columns(\n",
    "    [\n",
    "        rs.col(\"pois\")\n",
    "        .map_elements(\n",
    "            lambda s: [poi_geo_dict[s] for s in s],\n",
    "            return_dtype=rs.List(rs.String),\n",
    "        )\n",
    "        .alias(\"geohashes\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[\"dates\"][79].to_list()  # check out a temporal sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[\"TZs\"][79].to_list()  # ... and the corresponding timezones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The work *might* seem over, however, we still have timezones to account for, we want to normalize everything according to GMT, so we convert the timestamps accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "def UTC_to_local(utc, tz):\n",
    "\n",
    "    date = datetime.datetime.strptime(utc, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    date = date.replace(tzinfo=datetime.timezone.utc)\n",
    "\n",
    "    # shift by tz offset\n",
    "    date = date.astimezone(datetime.timezone(datetime.timedelta(minutes=tz)))\n",
    "\n",
    "    date_s = datetime.datetime.strftime(date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    return date_s\n",
    "\n",
    "\n",
    "def to_UNIX_time(date):\n",
    "    return datetime.datetime.strptime(\n",
    "        date, \"%Y-%m-%d %H:%M:%S\"\n",
    "    ).timestamp()  # we use UNIX time as a key for sorting the POIs in our polars query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UTC_to_local(\"Mon May 21 15:53:01 +0000 2012\", -420)  # example of usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.with_columns(\n",
    "    [\n",
    "        rs.struct([rs.col(\"dates\"), rs.col(\"TZs\")])\n",
    "        .alias(\"times\")\n",
    "        .map_elements(\n",
    "            lambda struct: [\n",
    "                UTC_to_local(date, tz)\n",
    "                for date, tz in zip(struct[\"dates\"], struct[\"TZs\"])\n",
    "            ],\n",
    "            return_dtype=rs.List(rs.String),\n",
    "        )\n",
    "    ]\n",
    ")  # This performs timezone conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sorted = final_data.select(  # sort the times\n",
    "    [\n",
    "        rs.col(\"user\"),\n",
    "        rs.struct(\n",
    "            [\n",
    "                rs.col(\"pois\"),\n",
    "                rs.col(\"times\"),\n",
    "            ]\n",
    "        ).map_elements(\n",
    "            lambda struct: [\n",
    "                poi\n",
    "                for poi, _ in sorted(\n",
    "                    zip(  # here we sort the POIs struct by UNIX timestamps of the GMT times\n",
    "                        struct[\"pois\"], [to_UNIX_time(date) for date in struct[\"times\"]]\n",
    "                    ),\n",
    "                    key=lambda s: s[1],\n",
    "                )\n",
    "            ],\n",
    "            return_dtype=rs.List(rs.String),\n",
    "        ),\n",
    "        rs.struct(\n",
    "            [\n",
    "                rs.col(\"geohashes\"),\n",
    "                rs.col(\"times\"),\n",
    "            ]\n",
    "        ).map_elements(\n",
    "            lambda struct: [\n",
    "                geo\n",
    "                for geo, _ in sorted(\n",
    "                    zip(\n",
    "                        struct[\"geohashes\"],  # same thing goes on for geohashes\n",
    "                        [to_UNIX_time(date) for date in struct[\"times\"]],\n",
    "                    ),\n",
    "                    key=lambda s: s[1],\n",
    "                )\n",
    "            ],\n",
    "            return_dtype=rs.List(rs.String),\n",
    "        ),\n",
    "        rs.col(\"times\")\n",
    "        .map_elements(\n",
    "            lambda dates: sorted(dates, key=to_UNIX_time),\n",
    "            return_dtype=rs.List(rs.String),\n",
    "        )\n",
    "        .alias(\"times_sorted\"),\n",
    "        rs.col(\"n_checkins\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# P.S, admittedly, it would have been more efficient to encode the geohashes *after* sorting the POIs,\n",
    "# so that we could save on the sorting of the geohashes. Tough luck, you can't win 'em all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now need to obtain a dataframe containing: each POI, it's geohash, and a set of all the check-ins it appears in\n",
    "this is just one `polars` query away!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_checkins = final_sorted.explode([\"pois\", \"geohashes\"]).drop(\"n_checkins\")\n",
    "\n",
    "pois_checkins = (\n",
    "    pois_checkins.with_columns(\n",
    "        [\n",
    "            rs.col(\"geohashes\").map_elements(lambda s: s[:4], rs.String).alias(\"g4\"),\n",
    "        ]\n",
    "    )\n",
    "    .drop(\"geohashes\")\n",
    "    .group_by([\"pois\", \"g4\"])\n",
    "    .agg([rs.col(\"times_sorted\").flatten().alias(\"checkin_times\")])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_checkins  # with this we can *efficiently* build our POI-POI spatial-temporal graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UTC_to_weekslot(utc: str) -> int:\n",
    "    \"\"\"UTC_to_weekslot converts a UTC timestamp to a weekslot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    utc : str\n",
    "        A string representing a UTC timestamp.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        A weekslot in the range [0, 56).\n",
    "    \"\"\"\n",
    "\n",
    "    date = datetime.datetime.strptime(utc, \"%Y-%m-%d %H:%M:%S\")\n",
    "    week = date.weekday()\n",
    "    hour = date.hour\n",
    "\n",
    "    return week * 8 + hour // 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to encode all of our inputs for our neural networks, this could *probably* be done \n",
    "with polars magic, but it's too delicate and we prefer classic for-looping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_dict = {\n",
    "    \"users\": LabelEncoder(),\n",
    "    \"pois\": LabelEncoder(),\n",
    "    \"g2\": LabelEncoder(),\n",
    "    \"g3\": LabelEncoder(),\n",
    "    \"g4\": LabelEncoder(),\n",
    "    \"g5\": LabelEncoder(),\n",
    "    \"g6\": LabelEncoder(),\n",
    "}\n",
    "\n",
    "encoded_data = {\n",
    "    \"users\": [],\n",
    "    \"pois\": [],\n",
    "    \"g2\": [],\n",
    "    \"g3\": [],\n",
    "    \"g4\": [],\n",
    "    \"g5\": [],\n",
    "    \"g6\": [],\n",
    "}\n",
    "\n",
    "unique_data = {\n",
    "    \"users\": set(),\n",
    "    \"pois\": set(),\n",
    "    \"g2\": set(),\n",
    "    \"g3\": set(),\n",
    "    \"g4\": set(),\n",
    "    \"g5\": set(),\n",
    "    \"g6\": set(),\n",
    "}\n",
    "\n",
    "# quick and dirty encoding:\n",
    "# 1. put every unique symbol in a list\n",
    "# 2. fit the respective encoder\n",
    "# 3. transform the lists\n",
    "\n",
    "for i, row in enumerate(final_sorted.iter_rows()):\n",
    "\n",
    "    user, pois, geohashes, times_sorted, n_checkins = row\n",
    "\n",
    "    g2 = [geo[:2] for geo in geohashes]\n",
    "    g3 = [geo[:3] for geo in geohashes]\n",
    "    g4 = [geo[:4] for geo in geohashes]\n",
    "    g5 = [geo[:5] for geo in geohashes]\n",
    "    g6 = [geo[:6] for geo in geohashes]  # redundant, but I like symmetry\n",
    "\n",
    "    unique_data[\"users\"].add(user)\n",
    "    unique_data[\"pois\"].update(pois)\n",
    "    unique_data[\"g2\"].update(g2)\n",
    "    unique_data[\"g3\"].update(g3)\n",
    "    unique_data[\"g4\"].update(g4)\n",
    "    unique_data[\"g5\"].update(g5)\n",
    "    unique_data[\"g6\"].update(g6)\n",
    "\n",
    "for property, enc, data in zip(\n",
    "    encoder_dict.keys(), encoder_dict.values(), unique_data.values()\n",
    "):\n",
    "    enc.fit(list(data))\n",
    "    encoder_dict[property] = enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this could be optimized, right now it takes a while, at least we have a nice progress bar to look at\n",
    "\n",
    "ds_size = len(final_sorted)\n",
    "\n",
    "for i, row in tqdm(enumerate(final_sorted.iter_rows()), total=ds_size):\n",
    "\n",
    "    user, pois, geohashes, times_sorted, n_checkins = row\n",
    "\n",
    "    g2 = [geo[:2] for geo in geohashes]\n",
    "    g3 = [geo[:3] for geo in geohashes]\n",
    "    g4 = [geo[:4] for geo in geohashes]\n",
    "    g5 = [geo[:5] for geo in geohashes]\n",
    "    g6 = [geo[:6] for geo in geohashes]\n",
    "\n",
    "    encoded_data[\"users\"].append(encoder_dict[\"users\"].transform([user])[0])\n",
    "    encoded_data[\"pois\"].append(encoder_dict[\"pois\"].transform(pois))\n",
    "    encoded_data[\"g2\"].append(encoder_dict[\"g2\"].transform(g2))\n",
    "    encoded_data[\"g3\"].append(encoder_dict[\"g3\"].transform(g3))\n",
    "    encoded_data[\"g4\"].append(encoder_dict[\"g4\"].transform(g4))\n",
    "    encoded_data[\"g5\"].append(encoder_dict[\"g5\"].transform(g5))\n",
    "    encoded_data[\"g6\"].append(encoder_dict[\"g6\"].transform(g6))\n",
    "\n",
    "    # sum 1 to all values to avoid 0s\n",
    "    encoded_data[\"users\"][-1] += 1\n",
    "    encoded_data[\"pois\"][-1] += 1\n",
    "    encoded_data[\"g2\"][-1] += 1\n",
    "    encoded_data[\"g3\"][-1] += 1\n",
    "    encoded_data[\"g4\"][-1] += 1\n",
    "    encoded_data[\"g5\"][-1] += 1\n",
    "    encoded_data[\"g6\"][-1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we left space for the padding token\n",
    "min((arr.min() for arr in encoded_data[\"pois\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois_checkins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also encode the graph dataframe so we can build the graphs\n",
    "\n",
    "pois_checkins = (\n",
    "    pois_checkins.lazy()\n",
    "    .with_columns(\n",
    "        [\n",
    "            rs.col(\"pois\").map_elements(\n",
    "                lambda s: encoder_dict[\"pois\"].transform([s])[0] + 1, rs.Int64\n",
    "            ),\n",
    "            rs.col(\"g4\").map_elements(\n",
    "                lambda s: encoder_dict[\"g4\"].transform([s])[0] + 1, rs.Int64\n",
    "            ),  # apply utc_to_weekslot to each timestamp in the list\n",
    "            rs.col(\"checkin_times\").map_elements(\n",
    "                lambda s: [UTC_to_weekslot(date) for date in s], rs.List(rs.Int64)\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    .sort(\"pois\")\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add fictitious POI 0 to the graph, with nonexistent geohash and no timeslot, so we get a 0 row and column for the padding token\n",
    "fake_datapoint = rs.DataFrame(\n",
    "    {\n",
    "        \"pois\": [0],\n",
    "        \"g4\": [pois_checkins[\"g4\"].max() + 42],\n",
    "        \"checkin_times\": [[43]],\n",
    "    }\n",
    ")\n",
    "# this is a lot of work since polars dataframes are immutable by default, we have to run a query to change the 43 into an empty list\n",
    "# we NEED the 43 otherwise polars won't infer the datatype of the list\n",
    "\n",
    "fake_datapoint = fake_datapoint.with_columns(\n",
    "    [rs.col(\"checkin_times\").map_elements(lambda s: [], rs.List(rs.Int64))]\n",
    ")\n",
    "\n",
    "pois_checkins = fake_datapoint.vstack(pois_checkins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_row = np.array(pois_checkins[\"g4\"].to_list()).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer product using equality\n",
    "spatial_graph = (spatial_row == spatial_row.T).astype(np.int32)\n",
    "spatial_graph[0, 0] = (\n",
    "    0  # the fake g4 is still equal to itself, we suppress this equality\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_row = pois_checkins[\"checkin_times\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_graph = np.zeros((spatial_row.shape[0], spatial_row.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_sets = [np.array(list(set(row))) for row in temporal_row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_sets = torch.zeros((len(temporal_sets), 56), dtype=torch.int8)\n",
    "\n",
    "for i, r in enumerate(temporal_row):\n",
    "    indices = torch.tensor(r, dtype=torch.long)\n",
    "    time_sets[i, indices] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_sets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AND outer product\n",
    "\n",
    "intersection = time_sets @ time_sets.T\n",
    "union = time_sets.unsqueeze(1) | time_sets.unsqueeze(0)\n",
    "union = union.sum(dim=2)\n",
    "iou = intersection / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_graph = iou >= 0.9\n",
    "# cast to int\n",
    "temporal_graph = temporal_graph.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_graph[0, :].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print information about the sparsity of the graphs, we note that \n",
    "the sparsity of the graphs is similar to that of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_density = (\n",
    "    temporal_graph.sum() / (temporal_graph.shape[0] * temporal_graph.shape[1])\n",
    ").item()\n",
    "spatial_density = (\n",
    "    spatial_graph.sum() / (spatial_graph.shape[0] * spatial_graph.shape[1])\n",
    ").item()\n",
    "\n",
    "print(f\"Temporal sparsity: {(1 - temporal_density) * 100:.2f}%\")\n",
    "\n",
    "print(f\"Spatial sparsity: {(1 - spatial_density) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Datamodule\n",
    "\n",
    "We then define a pytorch dataset and a custom collation function that allows us to dynamically\n",
    "pad sequences to the longest one in the batch (as opposed to the longest one in the dataset)\n",
    "as they are loaded during training, this gives us an edge in performance by dramatically reducing the \n",
    "sparsity of our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def rnn_collation_fn(batch):\n",
    "\n",
    "    users = []\n",
    "    pois = []\n",
    "    g2 = []\n",
    "    g3 = []\n",
    "    g4 = []\n",
    "    g5 = []\n",
    "    g6 = []\n",
    "\n",
    "    for user, poi, geo2, geo3, geo4, geo5, geo6 in batch:\n",
    "        users.append(user)\n",
    "        pois.append(poi)\n",
    "        g2.append(geo2)\n",
    "        g3.append(geo3)\n",
    "        g4.append(geo4)\n",
    "        g5.append(geo5)\n",
    "        g6.append(geo6)\n",
    "    seq = (\n",
    "        torch.tensor(users, dtype=torch.long),\n",
    "        pad_sequence(pois, batch_first=True, padding_value=0),\n",
    "        pad_sequence(g2, batch_first=True, padding_value=0),\n",
    "        pad_sequence(g3, batch_first=True, padding_value=0),\n",
    "        pad_sequence(g4, batch_first=True, padding_value=0),\n",
    "        pad_sequence(g5, batch_first=True, padding_value=0),\n",
    "        pad_sequence(g6, batch_first=True, padding_value=0),\n",
    "    )  # build a sequence\n",
    "\n",
    "    x = (\n",
    "        seq[0],\n",
    "        seq[1][:, :-1],\n",
    "        seq[2][:, :-1],\n",
    "        seq[3][:, :-1],\n",
    "        seq[4][:, :-1],\n",
    "        seq[5][:, :-1],\n",
    "        seq[6][:, :-1],\n",
    "    )  # omit the last one for sample\n",
    "\n",
    "    y = (\n",
    "        seq[0],\n",
    "        seq[1][:, 1:],\n",
    "        seq[2][:, 1:],\n",
    "        seq[3][:, 1:],\n",
    "        seq[4][:, 1:],\n",
    "        seq[5][:, 1:],\n",
    "        seq[6][:, 1:],\n",
    "    )  # omit the first one for ground truth\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "class CheckinDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"users\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        x = (\n",
    "            torch.tensor(encoded_data[\"users\"][idx], dtype=torch.long),\n",
    "            torch.tensor(encoded_data[\"pois\"][idx], dtype=torch.long),\n",
    "            torch.tensor(encoded_data[\"g2\"][idx], dtype=torch.long),\n",
    "            torch.tensor(encoded_data[\"g3\"][idx], dtype=torch.long),\n",
    "            torch.tensor(encoded_data[\"g4\"][idx], dtype=torch.long),\n",
    "            torch.tensor(encoded_data[\"g5\"][idx], dtype=torch.long),\n",
    "            torch.tensor(encoded_data[\"g6\"][idx], dtype=torch.long),\n",
    "        )\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckinModule(pl.LightningDataModule):\n",
    "    def __init__(self, encoded_data, batch_size=32, workers=4):\n",
    "        super().__init__()\n",
    "        self.encoded_data = encoded_data\n",
    "        self.batch_size = batch_size\n",
    "        self.workers = workers\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.whole_dataset = CheckinDataset(self.encoded_data)\n",
    "\n",
    "        l = len(self.whole_dataset)\n",
    "\n",
    "        train_size = int(0.8 * l)\n",
    "        val_size = int(0.1 * l)\n",
    "        test_size = l - train_size - val_size\n",
    "\n",
    "        # generate train, val, test datasets by random split\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = (\n",
    "            torch.utils.data.random_split(\n",
    "                self.whole_dataset, [train_size, val_size, test_size]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.workers,\n",
    "            collate_fn=rnn_collation_fn,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.workers,\n",
    "            collate_fn=rnn_collation_fn,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.workers,\n",
    "            collate_fn=rnn_collation_fn,\n",
    "        )\n",
    "\n",
    "    def save(self, whole_path, train_path, val_path, test_path):\n",
    "        torch.save(self.whole_dataset, whole_path)\n",
    "        torch.save(self.train_dataset, train_path)\n",
    "        torch.save(self.val_dataset, val_path)\n",
    "        torch.save(self.test_dataset, test_path)\n",
    "\n",
    "    @staticmethod  # load without instantiating\n",
    "    def load(whole_path, train_path, val_path, test_path):\n",
    "        whole_dataset = torch.load(whole_path)\n",
    "        train_dataset = torch.load(train_path)\n",
    "        val_dataset = torch.load(val_path)\n",
    "        test_dataset = torch.load(test_path)\n",
    "        return whole_dataset, train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BaselineDimensions:\n",
    "    nuser: int\n",
    "    npoi: int\n",
    "    g2len: int\n",
    "    g3len: int\n",
    "    g4len: int\n",
    "    g5len: int\n",
    "    g6len: int\n",
    "\n",
    "\n",
    "# HMT_RN (Hierarchical Multi-Task Recurrent Network)\n",
    "class HMT_RN(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimensions: BaselineDimensions,\n",
    "        embedding_dim,\n",
    "        lstm_hidden_dim,\n",
    "        dropout_rate=0.9,  # 0.9 is a lot, but the paper says so.\n",
    "    ):\n",
    "        super(HMT_RN, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = lstm_hidden_dim\n",
    "        self.dims = dimensions\n",
    "\n",
    "        # Embedding layers one for user, one for poi and one for each G@P\n",
    "        self.user_embedding = nn.Embedding(\n",
    "            dimensions.nuser, embedding_dim, padding_idx=0\n",
    "        )\n",
    "        self.poi_embedding = nn.Embedding(dimensions.npoi, embedding_dim, padding_idx=0)\n",
    "        self.g2_embed = nn.Embedding(dimensions.g2len, embedding_dim, padding_idx=0)\n",
    "        self.g3_embed = nn.Embedding(dimensions.g3len, embedding_dim, padding_idx=0)\n",
    "        self.g4_embed = nn.Embedding(dimensions.g4len, embedding_dim, padding_idx=0)\n",
    "        self.g5_embed = nn.Embedding(dimensions.g5len, embedding_dim, padding_idx=0)\n",
    "        self.g6_embed = nn.Embedding(dimensions.g6len, embedding_dim, padding_idx=0)\n",
    "\n",
    "        # Dropout layer for embeddings\n",
    "        self.e_drop = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim * 2, hidden_size=lstm_hidden_dim, batch_first=True\n",
    "        )\n",
    "\n",
    "        # Linear layers for prediction tasks\n",
    "        self.linear_poi = nn.Linear(lstm_hidden_dim + embedding_dim, dimensions.npoi)\n",
    "        self.linear_g2 = nn.Linear(lstm_hidden_dim + embedding_dim, dimensions.g2len)\n",
    "        self.linear_g3 = nn.Linear(lstm_hidden_dim + embedding_dim, dimensions.g3len)\n",
    "        self.linear_g4 = nn.Linear(lstm_hidden_dim + embedding_dim, dimensions.g4len)\n",
    "        self.linear_g5 = nn.Linear(lstm_hidden_dim + embedding_dim, dimensions.g5len)\n",
    "        self.linear_g6 = nn.Linear(lstm_hidden_dim + embedding_dim, dimensions.g6len)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, w):\n",
    "\n",
    "        if type(w) == nn.Linear:\n",
    "            nn.init.kaiming_normal_(w.weight)\n",
    "            nn.init.constant_(w.bias, 0)\n",
    "        elif type(w) == nn.LSTM:\n",
    "            for name, param in w.named_parameters():\n",
    "                if \"bias\" in name:\n",
    "                    nn.init.constant_(param, 0)\n",
    "                elif \"weight\" in name:\n",
    "                    nn.init.kaiming_normal_(param)\n",
    "        elif type(w) == nn.Embedding:\n",
    "            nn.init.kaiming_normal_(w.weight)\n",
    "            nn.init.constant_(w.weight[0], 0)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"forward passes the batch through the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch : `tuple[torch.Tensor]`\n",
    "            A tuple of tensors ordered as follows:\n",
    "            (users, poi, x_geoHash2, x_geoHash3, x_geoHash4, x_geoHash5, x_geoHash6)\n",
    "        \"\"\"\n",
    "\n",
    "        users, poi, x_geoHash2, x_geoHash3, x_geoHash4, x_geoHash5, x_geoHash6 = batch\n",
    "\n",
    "        B, T = poi.shape\n",
    "\n",
    "        # make it so  that users are tiled T times\n",
    "        users = users.repeat(T, 1).T\n",
    "\n",
    "        e_user = self.e_drop(self.user_embedding(users))\n",
    "        e_poi = self.e_drop(self.poi_embedding(poi))\n",
    "        e_gap2 = self.e_drop(self.g2_embed(x_geoHash2))\n",
    "        e_gap3 = self.e_drop(self.g3_embed(x_geoHash3))\n",
    "        e_gap4 = self.e_drop(self.g4_embed(x_geoHash4))\n",
    "        e_gap5 = self.e_drop(self.g5_embed(x_geoHash5))\n",
    "        e_gap6 = self.e_drop(self.g6_embed(x_geoHash6))\n",
    "\n",
    "        h_t, c_t = self.lstm(torch.cat((e_user, e_poi), dim=2))\n",
    "\n",
    "        # dense layers\n",
    "        next_poi = self.linear_poi(torch.cat((h_t, e_poi), dim=2))\n",
    "        next_g2 = self.linear_g2(torch.cat((h_t, e_gap2), dim=2))\n",
    "        next_g3 = self.linear_g3(torch.cat((h_t, e_gap3), dim=2))\n",
    "        next_g4 = self.linear_g4(torch.cat((h_t, e_gap4), dim=2))\n",
    "        next_g5 = self.linear_g5(torch.cat((h_t, e_gap5), dim=2))\n",
    "        next_g6 = self.linear_g6(torch.cat((h_t, e_gap6), dim=2))\n",
    "\n",
    "        return next_poi, next_g2, next_g3, next_g4, next_g5, next_g6\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        (\n",
    "            poi_pred,\n",
    "            gap2_pred,\n",
    "            gap3_pred,\n",
    "            gap4_pred,\n",
    "            gap5_pred,\n",
    "            gap6_pred,\n",
    "        ) = self(x)\n",
    "\n",
    "        loss_poi = self.criterion(\n",
    "            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1)\n",
    "        )\n",
    "        loss_gap2 = self.criterion(\n",
    "            gap2_pred.reshape(-1, self.dims.g2len), y[2].reshape(-1)\n",
    "        )\n",
    "        loss_gap3 = self.criterion(\n",
    "            gap3_pred.reshape(-1, self.dims.g3len), y[3].reshape(-1)\n",
    "        )\n",
    "        loss_gap4 = self.criterion(\n",
    "            gap4_pred.reshape(-1, self.dims.g4len), y[4].reshape(-1)\n",
    "        )\n",
    "        loss_gap5 = self.criterion(\n",
    "            gap5_pred.reshape(-1, self.dims.g5len), y[5].reshape(-1)\n",
    "        )\n",
    "        loss_gap6 = self.criterion(\n",
    "            gap6_pred.reshape(-1, self.dims.g6len), y[6].reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            loss_poi + loss_gap2 + loss_gap3 + loss_gap4 + loss_gap5 + loss_gap6\n",
    "        ) / 6\n",
    "        self.log(\"train/loss\", loss)\n",
    "        self.log(\"train/loss_gap2\", loss_gap2)\n",
    "        self.log(\"train/loss_gap3\", loss_gap3)\n",
    "        self.log(\"train/loss_gap4\", loss_gap4)\n",
    "        self.log(\"train/loss_gap5\", loss_gap5)\n",
    "        self.log(\"train/loss_gap6\", loss_gap6)\n",
    "        self.log(\"train/loss_poi\", loss_poi)\n",
    "\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        (\n",
    "            poi_pred,\n",
    "            gap2_pred,\n",
    "            gap3_pred,\n",
    "            gap4_pred,\n",
    "            gap5_pred,\n",
    "            gap6_pred,\n",
    "        ) = self(x)\n",
    "\n",
    "        loss_poi = self.criterion(\n",
    "            poi_pred.reshape(-1, self.dims.npoi), y[1].reshape(-1)\n",
    "        )\n",
    "        loss_gap2 = self.criterion(\n",
    "            gap2_pred.reshape(-1, self.dims.g2len), y[2].reshape(-1)\n",
    "        )\n",
    "        loss_gap3 = self.criterion(\n",
    "            gap3_pred.reshape(-1, self.dims.g3len), y[3].reshape(-1)\n",
    "        )\n",
    "        loss_gap4 = self.criterion(\n",
    "            gap4_pred.reshape(-1, self.dims.g4len), y[4].reshape(-1)\n",
    "        )\n",
    "        loss_gap5 = self.criterion(\n",
    "            gap5_pred.reshape(-1, self.dims.g5len), y[5].reshape(-1)\n",
    "        )\n",
    "        loss_gap6 = self.criterion(\n",
    "            gap6_pred.reshape(-1, self.dims.g6len), y[6].reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss = (\n",
    "            loss_poi + loss_gap2 + loss_gap3 + loss_gap4 + loss_gap5 + loss_gap6\n",
    "        ) / 6\n",
    "\n",
    "        self.log(\"val/loss\", loss)\n",
    "        self.log(\"val/loss_gap2\", loss_gap2)\n",
    "        self.log(\"val/loss_gap3\", loss_gap3)\n",
    "        self.log(\"val/loss_gap4\", loss_gap4)\n",
    "        self.log(\"val/loss_gap5\", loss_gap5)\n",
    "        self.log(\"val/loss_gap6\", loss_gap6)\n",
    "        self.log(\"val/loss_poi\", loss_poi)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define optimizer and scheduler\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=2e-4, amsgrad=True)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN Components\n",
    "\n",
    "\n",
    "class attn_LSTM(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(attn_LSTM, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W = nn.Linear(embedding_dim, 4 * hidden_dim)\n",
    "        self.U = nn.Linear(hidden_dim, 4 * hidden_dim)\n",
    "\n",
    "        self.s_W = nn.Linear(embedding_dim, 4 * hidden_dim)\n",
    "        self.t_W = nn.Linear(embedding_dim, 4 * hidden_dim)\n",
    "\n",
    "    def forward(self, x, hidden, spatial, temporal, numTimeSteps):\n",
    "        h_t, c_t = hidden\n",
    "\n",
    "        previous_h_t = h_t\n",
    "        previous_c_t = c_t\n",
    "\n",
    "        allGates_preact = (\n",
    "            self.W(x) + self.U(previous_h_t) + self.s_W(spatial) + self.t_W(temporal)\n",
    "        )\n",
    "\n",
    "        input_g = allGates_preact[:, :, : self.hidden_dim].sigmoid()\n",
    "        forget_g = allGates_preact[\n",
    "            :, :, self.hidden_dim : 2 * self.hidden_dim\n",
    "        ].sigmoid()\n",
    "        output_g = allGates_preact[\n",
    "            :, :, 2 * self.hidden_dim : 3 * self.hidden_dim\n",
    "        ].sigmoid()\n",
    "        c_t_g = allGates_preact[:, :, 3 * self.hidden_dim :].tanh()\n",
    "\n",
    "        c_t = forget_g * previous_c_t + input_g * c_t_g\n",
    "        h_t = output_g * c_t.tanh()\n",
    "\n",
    "        batchSize = x.shape[0]\n",
    "        h_t = h_t.view(batchSize, numTimeSteps, self.hidden_dim)\n",
    "        c_t = c_t.view(batchSize, numTimeSteps, self.hidden_dim)\n",
    "\n",
    "        return h_t, c_t\n",
    "\n",
    "\n",
    "def get_neighbours(adj_matrix, poi):\n",
    "    neigh_indices_list = []\n",
    "    max_length = 0\n",
    "\n",
    "    for batch_poi in poi:\n",
    "        batch_indices = []\n",
    "        for single_poi in batch_poi:\n",
    "            poi_row = adj_matrix[single_poi]\n",
    "            neigh_indices = torch.where(poi_row == 1)[0]\n",
    "            batch_indices.append(neigh_indices)\n",
    "            max_length = max(max_length, len(neigh_indices))\n",
    "\n",
    "        neigh_indices_list.append(batch_indices)\n",
    "\n",
    "    padded_neigh_indices_list = []\n",
    "    for batch_indices in neigh_indices_list:\n",
    "        padded_batch_indices = pad_sequence(\n",
    "            batch_indices, batch_first=True, padding_value=0\n",
    "        )\n",
    "        padded_neigh_indices_list.append(padded_batch_indices)\n",
    "\n",
    "    padded_tensor = torch.stack(padded_neigh_indices_list)\n",
    "\n",
    "    return padded_tensor\n",
    "\n",
    "\n",
    "class GRNSelfAttention(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, n_heads):\n",
    "\n",
    "        super(GRNSelfAttention, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.Wp = nn.Linear(hidden_dim, hidden_dim)  # embeddings to pre-concat\n",
    "        self.Wa = nn.Linear(2 * hidden_dim, hidden_dim)  # concatenation to pre-softmax\n",
    "\n",
    "        # total size = 3 * (hidden_dim) ** 2, quadratic in embedding size\n",
    "\n",
    "    def forward(self, poi, neighbors):\n",
    "        \"\"\"forward\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        poi: torch.Tensor\n",
    "            A batched tensor of embedded POI vectors, (B x H) where H is the\n",
    "            embedding dimension\n",
    "        neighbors: torch.Tensor\n",
    "            A batched tensor of sequences of embedded POI vectors that are extracted\n",
    "            from an adjacency matrix (temporal or spatial neighbors of POI),\n",
    "            (N x B x H), where N is the number of neighbours of POI, B is the\n",
    "            batch size, H is the embedding dimension, and must be the same as POI\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[torch.Tensor, torch.Tensor]\n",
    "          A tuple containing the self-attention weighted hadamard product of neighbour activations\n",
    "          in the first index, the attention weights in the second index.\n",
    "        \"\"\"\n",
    "        # assert len(poi.shape) == 2, f\"POI tensor must be 2D, got {poi.shape} instead\"\n",
    "        assert (\n",
    "            len(neighbors.shape) == 3\n",
    "        ), f\"Neighbour tensor must be 3D, got {neighbors.shape} instead\"\n",
    "\n",
    "        neighbors = neighbors.half()\n",
    "        poi = poi.half()\n",
    "        T, B, H = neighbors.shape\n",
    "\n",
    "        h_poi = self.Wp(poi)\n",
    "        h_n = self.Wp(neighbors)\n",
    "        h_cat = torch.cat([h_poi.expand(T, -1, -1), h_n], dim=2)\n",
    "        h_att = F.leaky_relu(self.Wa(h_cat))\n",
    "\n",
    "        alpha = torch.nn.functional.softmax(h_att, dim=1)\n",
    "\n",
    "        p = torch.sum(alpha * h_n, dim=0)\n",
    "        return p, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRN (Graph Recurrent Network)\n",
    "class GRN(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims: BaselineDimensions,\n",
    "        spatial_graph,\n",
    "        temporal_graph,\n",
    "        hidden_dim,\n",
    "        n_heads,\n",
    "        dropout_rate=0.9,\n",
    "    ):\n",
    "        super(GRN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.dims = dims\n",
    "\n",
    "        self.spatial_graph = spatial_graph\n",
    "        self.temporal_graph = temporal_graph\n",
    "\n",
    "        self.spatial_attn = GRNSelfAttention(hidden_dim, n_heads)\n",
    "        self.temporal_attn = GRNSelfAttention(hidden_dim, n_heads)\n",
    "\n",
    "        self.spatial_lstm = attn_LSTM(hidden_dim, hidden_dim)\n",
    "        self.temporal_lstm = attn_LSTM(hidden_dim, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.linear_poi = nn.Linear(hidden_dim, dims.npoi)\n",
    "        self.linear_g2 = nn.Linear(hidden_dim, dims.g2len)\n",
    "        self.linear_g3 = nn.Linear(hidden_dim, dims.g3len)\n",
    "        self.linear_g4 = nn.Linear(hidden_dim, dims.g4len)\n",
    "        self.linear_g5 = nn.Linear(hidden_dim, dims.g5len)\n",
    "        self.linear_g6 = nn.Linear(hidden_dim, dims.g6len)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def init_weights(self, w):\n",
    "        if type(w) == nn.Linear:\n",
    "            nn.init.kaiming_normal_(w.weight)\n",
    "            nn.init.constant_(w.bias, 0)\n",
    "        elif type(w) == nn.LSTM:\n",
    "            for name, param in w.named_parameters():\n",
    "                if \"bias\" in name:\n",
    "                    nn.init.constant_(param, 0)\n",
    "                elif \"weight\" in name:\n",
    "                    nn.init.kaiming_normal_(param)\n",
    "        elif type(w) == nn.Embedding:\n",
    "            nn.init.kaiming_normal_(w.weight)\n",
    "            nn.init.constant_(w.weight[0], 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        users, poi, x_geoHash2, x_geoHash3, x_geoHash4, x_geoHash5, x_geoHash6 = x\n",
    "\n",
    "        B, T = poi.shape\n",
    "\n",
    "        users = users.repeat(T, 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = encoder_dict[\"users\"].classes_.shape[0]\n",
    "n_pois = encoder_dict[\"pois\"].classes_.shape[0]\n",
    "n_g2 = encoder_dict[\"g2\"].classes_.shape[0]\n",
    "n_g3 = encoder_dict[\"g3\"].classes_.shape[0]\n",
    "n_g4 = encoder_dict[\"g4\"].classes_.shape[0]\n",
    "n_g5 = encoder_dict[\"g5\"].classes_.shape[0]\n",
    "n_g6 = encoder_dict[\"g6\"].classes_.shape[0]\n",
    "\n",
    "\n",
    "# account for the padding token\n",
    "dims = BaselineDimensions(\n",
    "    n_users + 1, n_pois + 1, n_g2 + 1, n_g3 + 1, n_g4 + 1, n_g5 + 1, n_g6 + 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch import Trainer\n",
    "\n",
    "\n",
    "batch_size = 60\n",
    "wandb.finish()\n",
    "torch.cuda.empty_cache()\n",
    "# cargo-cult like stuff that is supposed to make you faster\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "ds = CheckinModule(encoded_data, batch_size=32, workers=4)\n",
    "\n",
    "wandb.init(project=\"trovailpoi\")\n",
    "\n",
    "classifier = HMT_RN(dims, embedding_dim=1024, lstm_hidden_dim=1024)\n",
    "wandb_logger = WandbLogger(project=\"trovailpoi\")\n",
    "trainer = Trainer(\n",
    "    max_epochs=40,\n",
    "    accelerator=\"auto\",\n",
    "    devices=[0],\n",
    "    log_every_n_steps=10,\n",
    "    logger=wandb_logger,\n",
    "    strategy=\"auto\",\n",
    "    callbacks=[\n",
    "        torchpl.callbacks.LearningRateMonitor(logging_interval=\"step\"),\n",
    "        torchpl.callbacks.ModelCheckpoint(\n",
    "            monitor=\"val/loss\",\n",
    "            mode=\"min\",\n",
    "            save_top_k=1,\n",
    "            save_last=True,\n",
    "            filename=\"best_model\",\n",
    "        ),\n",
    "        torchpl.callbacks.EarlyStopping(\n",
    "            monitor=\"val/loss\", patience=3, min_delta=0.001, mode=\"min\"\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "\n",
    "trainer.fit(model=classifier, datamodule=ds)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapbook for Experimentation\n",
    "\n",
    "Ignore all code below, it's just for quick prototyping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
